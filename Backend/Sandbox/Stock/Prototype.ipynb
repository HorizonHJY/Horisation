{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T20:44:19.015627Z",
     "start_time": "2025-09-23T20:44:18.418078Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === é…ç½®è·¯å¾„ | Paths ===\n",
    "input_dir = Path(\"Data\")                 # ä½ çš„ Excel æ–‡ä»¶ç›®å½•\n",
    "output_path = input_dir / \"fintech_indices_merged.xlsx\"  # åˆå¹¶åè¾“å‡º\n",
    "\n",
    "# === å·¥å…·å‡½æ•°ï¼šæ¸…æ´—å¹¶å»é‡ sheet å | Sheet-name sanitizer ===\n",
    "def sanitize_sheet_name(name: str) -> str:\n",
    "    # Excel ä¸å…è®¸çš„å­—ç¬¦ï¼š : \\ / ? * [ ]\n",
    "    name = re.sub(r'[:\\\\/?*\\[\\]]', '-', name)\n",
    "    name = name.strip()\n",
    "    # å¤„ç†ç©ºå­—ç¬¦ä¸²æƒ…å†µ\n",
    "    if not name:\n",
    "        name = \"Unnamed\"\n",
    "    return name[:31] if len(name) > 31 else name\n",
    "\n",
    "excel_files = sorted([p for p in input_dir.glob(\"*.xls*\")\n",
    "                      if p.suffix.lower() in {\".xlsx\", \".xlsm\", \".xls\"}])\n",
    "if not excel_files:\n",
    "    raise FileNotFoundError(f\"No Excel files found in: {input_dir.resolve()}\")\n",
    "\n",
    "used_sheet_names = set()\n",
    "all_names = []\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    for fpath in excel_files:\n",
    "        try:\n",
    "            xfile = pd.ExcelFile(fpath)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Warning: Could not read {fpath.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sname in xfile.sheet_names:\n",
    "            try:\n",
    "                df = xfile.parse(sname)\n",
    "\n",
    "                # è·³è¿‡ç©ºè¡¨\n",
    "                if df.empty:\n",
    "                    print(f\"âš ï¸  Warning: Sheet '{sname}' in {fpath.name} is empty, skipping\")\n",
    "                    continue\n",
    "\n",
    "                base = f\"{fpath.stem}-{sname}\"\n",
    "                sheet_name = sanitize_sheet_name(base)\n",
    "                original_sheet_name = sheet_name\n",
    "                if sheet_name in used_sheet_names:\n",
    "                    i = 1\n",
    "                    while True:\n",
    "                        candidate = sanitize_sheet_name(f\"{original_sheet_name}-{i}\")\n",
    "                        if candidate not in used_sheet_names:\n",
    "                            sheet_name = candidate\n",
    "                            break\n",
    "                        i += 1\n",
    "                        if i > 100:  # å®‰å…¨é™åˆ¶\n",
    "                            sheet_name = sanitize_sheet_name(f\"{original_sheet_name}-dup\")\n",
    "                            break\n",
    "\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                used_sheet_names.add(sheet_name)\n",
    "\n",
    "                # --- ä»è¯¥è¡¨æŠ½å– Name åˆ—ï¼ŒåŠ å…¥ç»Ÿè®¡æ±  | collect 'Name' for summary\n",
    "                # å®¹é”™åŒ¹é…ï¼ˆå¤§å°å†™/ç©ºæ ¼ï¼‰ | tolerant match\n",
    "                cols_norm = {c: str(c).strip().lower() for c in df.columns}\n",
    "                name_col = next((orig for orig, low in cols_norm.items()\n",
    "                               if low in [\"name\", \"åç§°\", \"å§“å\"]), None)\n",
    "\n",
    "                if name_col is not None:\n",
    "                    names = (df[name_col]\n",
    "                             .dropna()\n",
    "                             .astype(str)\n",
    "                             .str.strip()\n",
    "                             .replace({\"\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
    "                             .dropna())\n",
    "                    # ç»Ÿä¸€å¤§å°å†™é¿å…åŒåå¤§å°å†™å·®å¼‚ | normalize case to merge duplicates\n",
    "                    if not names.empty:\n",
    "                        names = names.str.upper()\n",
    "                        all_names.append(names)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Warning: Could not process sheet '{sname}' in {fpath.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # === ç”Ÿæˆ Summary å­è¡¨ | Build and write Summary sheet ===\n",
    "    if all_names:\n",
    "        try:\n",
    "            all_names_series = pd.concat(all_names, ignore_index=True)\n",
    "            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äººåçš„æ¡ç›®ï¼ˆå¦‚æ•°å­—ã€è¿‡çŸ­å­—ç¬¦ä¸²ï¼‰\n",
    "            valid_names = all_names_series[all_names_series.str.len() >= 2]  # è‡³å°‘2ä¸ªå­—ç¬¦\n",
    "            if not valid_names.empty:\n",
    "                summary = (valid_names.value_counts()\n",
    "                           .rename_axis(\"Name\")\n",
    "                           .reset_index(name=\"Count\")\n",
    "                           .sort_values([\"Count\", \"Name\"], ascending=[False, True]))\n",
    "                summary.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "            else:\n",
    "                print(\"âš ï¸  Warning: No valid names found for Summary sheet\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Warning: Could not create Summary sheet: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Warning: No name data found for Summary sheet\")\n",
    "\n",
    "print(f\"âœ… Done. Merged {len(used_sheet_names)} sheets -> {output_path}\")\n",
    "if all_names:\n",
    "    all_names_series = pd.concat(all_names, ignore_index=True)\n",
    "    print(f\"ğŸ“Š Summary: {len(all_names_series)} total name entries\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Warning: Could not read fintech_indices_merged.xlsx: Excel file format cannot be determined, you must specify an engine manually.\n",
      "âœ… Done. Merged 7 sheets -> Data/fintech_indices_merged.xlsx\n",
      "ğŸ“Š Summary: 390 total name entries\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:37:56.572679Z",
     "start_time": "2025-10-02T22:37:40.922774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "stock_mapping = [\n",
    "    (\"Cryptocurrency\", \"COINBASE GLOBAL INC -CLASS A\", \"COIN\"),\n",
    "    (\"Cryptocurrency\", \"BLOCK INC\", \"XYZ\"),\n",
    "    (\"Cryptocurrency\", \"RIOT PLATFORMS INC\", \"RIOT\"),\n",
    "    (\"Cryptocurrency\", \"CIPHER MINING INC\", \"CIFR\"),\n",
    "    (\"Cryptocurrency\", \"MARATHON HOLDINGS INC\", \"MARA\"),\n",
    "    (\"Credit Card & BNPL\", \"AFFIRM HOLDINGS INC\", \"AFRM\"),\n",
    "    (\"Credit Card & BNPL\", \"AFTERPAY\", \"APTY\"),\n",
    "    (\"Credit Card & BNPL\", \"MARQETA INC-A\", \"MQ\"),\n",
    "    (\"Credit Card & BNPL\", \"BREAD FINANCIAL HOLDINGS INC\", \"BFH\"),\n",
    "    (\"Credit Card & BNPL\", \"SEZZLE INC\", \"SEZL\"),\n",
    "    (\"Fintech Hardware\", \"TOAST INC-CLASS A\", \"TOST\"),\n",
    "    (\"Fintech Hardware\", \"SHIFT4 PAYMENTS INC\", \"FOUR\"),\n",
    "    (\"Fintech Hardware\", \"VERIFONE\", \"VERI\"),\n",
    "    (\"Fintech Hardware\", \"PAX GLOBAL TECHNOLOGY LTD\", \"PAX\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"INTUIT INC\", \"INTU\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"SS&C TECHNOLOGIES HOLDINGS\", \"SSNC\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"BLACKLINE INC\", \"BL\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FIDELITY NATIONAL INFO SERV (FIS)\", \"FIS\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FISERV INC\", \"FI\"),\n",
    "]\n",
    "map_df = pd.DataFrame(stock_mapping, columns=[\"Sub Theme\", \"Company\", \"Ticker\"])\n",
    "\n",
    "tickers = map_df[\"Ticker\"].unique().tolist()\n",
    "# chunks = [tickers[i:i+2] for i in range(0, len(tickers), 2)]\n",
    "data_list = []\n",
    "for ticker in tickers:\n",
    "    data = yf.download(ticker, start=\"2023-01-01\", end=\"2025-10-01\", interval=\"1d\", auto_adjust=False, threads=False)\n",
    "    data_list.append(data)\n",
    "prices = pd.concat(data_list, axis=1)\n",
    "\n",
    "\n",
    "df = prices.stack(level=0).reset_index()\n",
    "df = df.rename(columns={\"level_0\": \"Date\", \"level_1\": \"Ticker\"})\n",
    "df = df.merge(map_df, on=\"Ticker\", how=\"left\")\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "df[\"r_t\"] = df.groupby(\"Ticker\")[\"Adj Close\"].transform(lambda s: np.log(s / s.shift(1)))\n",
    "df = df.dropna(subset=[\"r_t\"])\n",
    "\n",
    "\n",
    "cols_front = [\"Date\", \"Sub Theme\", \"Company\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"r_t\"]\n",
    "df = df[[c for c in cols_front if c in df.columns]]\n",
    "df.to_csv('test.csv', index=False)\n",
    "print(df.head(12))"
   ],
   "id": "109c7380f0fa4e9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['COIN']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['XYZ']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['RIOT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['CIFR']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MARA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AFRM']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['APTY']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MQ']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BFH']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SEZL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['TOST']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['FOUR']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['VERI']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['PAX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['INTU']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SSNC']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['FIS']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['FI']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "C:\\Users\\Horiz\\AppData\\Local\\Temp\\ipykernel_19056\\3039779751.py:38: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  df = prices.stack(level=0).reset_index()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Ticker'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     38\u001B[39m df = prices.stack(level=\u001B[32m0\u001B[39m).reset_index()\n\u001B[32m     39\u001B[39m df = df.rename(columns={\u001B[33m\"\u001B[39m\u001B[33mlevel_0\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mlevel_1\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mTicker\u001B[39m\u001B[33m\"\u001B[39m})\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m df = \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmap_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTicker\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mleft\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m df = df.sort_values([\u001B[33m\"\u001B[39m\u001B[33mTicker\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     42\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mr_t\u001B[39m\u001B[33m\"\u001B[39m] = df.groupby(\u001B[33m\"\u001B[39m\u001B[33mTicker\u001B[39m\u001B[33m\"\u001B[39m)[\u001B[33m\"\u001B[39m\u001B[33mAdj Close\u001B[39m\u001B[33m\"\u001B[39m].transform(\u001B[38;5;28;01mlambda\u001B[39;00m s: np.log(s / s.shift(\u001B[32m1\u001B[39m)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\pandas\\core\\frame.py:10839\u001B[39m, in \u001B[36mDataFrame.merge\u001B[39m\u001B[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[39m\n\u001B[32m  10820\u001B[39m \u001B[38;5;129m@Substitution\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m  10821\u001B[39m \u001B[38;5;129m@Appender\u001B[39m(_merge_doc, indents=\u001B[32m2\u001B[39m)\n\u001B[32m  10822\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmerge\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m  10835\u001B[39m     validate: MergeValidate | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m  10836\u001B[39m ) -> DataFrame:\n\u001B[32m  10837\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreshape\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmerge\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m merge\n\u001B[32m> \u001B[39m\u001B[32m10839\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m  10840\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m  10841\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10843\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10844\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10845\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10846\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10847\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10848\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10849\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10850\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10851\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10852\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m  10853\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001B[39m, in \u001B[36mmerge\u001B[39m\u001B[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[39m\n\u001B[32m    155\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _cross_merge(\n\u001B[32m    156\u001B[39m         left_df,\n\u001B[32m    157\u001B[39m         right_df,\n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m         copy=copy,\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     op = \u001B[43m_MergeOperation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_on\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mleft_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mright_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuffixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindicator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m op.get_result(copy=copy)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:794\u001B[39m, in \u001B[36m_MergeOperation.__init__\u001B[39m\u001B[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[39m\n\u001B[32m    784\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m MergeError(msg)\n\u001B[32m    786\u001B[39m \u001B[38;5;28mself\u001B[39m.left_on, \u001B[38;5;28mself\u001B[39m.right_on = \u001B[38;5;28mself\u001B[39m._validate_left_right_on(left_on, right_on)\n\u001B[32m    788\u001B[39m (\n\u001B[32m    789\u001B[39m     \u001B[38;5;28mself\u001B[39m.left_join_keys,\n\u001B[32m    790\u001B[39m     \u001B[38;5;28mself\u001B[39m.right_join_keys,\n\u001B[32m    791\u001B[39m     \u001B[38;5;28mself\u001B[39m.join_names,\n\u001B[32m    792\u001B[39m     left_drop,\n\u001B[32m    793\u001B[39m     right_drop,\n\u001B[32m--> \u001B[39m\u001B[32m794\u001B[39m ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_merge_keys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m left_drop:\n\u001B[32m    797\u001B[39m     \u001B[38;5;28mself\u001B[39m.left = \u001B[38;5;28mself\u001B[39m.left._drop_labels_or_levels(left_drop)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1311\u001B[39m, in \u001B[36m_MergeOperation._get_merge_keys\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1307\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m lk \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1308\u001B[39m     \u001B[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001B[39;00m\n\u001B[32m   1309\u001B[39m     \u001B[38;5;66;03m#  the latter of which will raise\u001B[39;00m\n\u001B[32m   1310\u001B[39m     lk = cast(Hashable, lk)\n\u001B[32m-> \u001B[39m\u001B[32m1311\u001B[39m     left_keys.append(\u001B[43mleft\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_get_label_or_level_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlk\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1312\u001B[39m     join_names.append(lk)\n\u001B[32m   1313\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1314\u001B[39m     \u001B[38;5;66;03m# work-around for merge_asof(left_index=True)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\pandas\\core\\generic.py:1911\u001B[39m, in \u001B[36mNDFrame._get_label_or_level_values\u001B[39m\u001B[34m(self, key, axis)\u001B[39m\n\u001B[32m   1909\u001B[39m     values = \u001B[38;5;28mself\u001B[39m.axes[axis].get_level_values(key)._values\n\u001B[32m   1910\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1911\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[32m   1913\u001B[39m \u001B[38;5;66;03m# Check for duplicates\u001B[39;00m\n\u001B[32m   1914\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m values.ndim > \u001B[32m1\u001B[39m:\n",
      "\u001B[31mKeyError\u001B[39m: 'Ticker'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T23:07:01.763513Z",
     "start_time": "2025-10-02T23:06:22.731222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ä½ çš„æ˜ å°„è¡¨ map_df ä¿æŒä¸è®Š\n",
    "stock_mapping = [\n",
    "    (\"Cryptocurrency\", \"COINBASE GLOBAL INC -CLASS A\", \"COIN\"),\n",
    "    (\"Cryptocurrency\", \"BLOCK INC\", \"XYZ\"),\n",
    "    (\"Cryptocurrency\", \"RIOT PLATFORMS INC\", \"RIOT\"),\n",
    "    (\"Cryptocurrency\", \"CIPHER MINING INC\", \"CIFR\"),\n",
    "    (\"Cryptocurrency\", \"MARATHON HOLDINGS INC\", \"MARA\"),\n",
    "    (\"Credit Card & BNPL\", \"AFFIRM HOLDINGS INC\", \"AFRM\"),\n",
    "    (\"Credit Card & BNPL\", \"AFTERPAY\", \"APTY\"),\n",
    "    (\"Credit Card & BNPL\", \"MARQETA INC-A\", \"MQ\"),\n",
    "    (\"Credit Card & BNPL\", \"BREAD FINANCIAL HOLDINGS INC\", \"BFH\"),\n",
    "    (\"Credit Card & BNPL\", \"SEZZLE INC\", \"SEZL\"),\n",
    "    (\"Fintech Hardware\", \"TOAST INC-CLASS A\", \"TOST\"),\n",
    "    (\"Fintech Hardware\", \"SHIFT4 PAYMENTS INC\", \"FOUR\"),\n",
    "    (\"Fintech Hardware\", \"VERIFONE\", \"VERI\"),\n",
    "    (\"Fintech Hardware\", \"PAX GLOBAL TECHNOLOGY LTD\", \"PAX\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"INTUIT INC\", \"INTU\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"SS&C TECHNOLOGIES HOLDINGS\", \"SSNC\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"BLACKLINE INC\", \"BL\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FIDELITY NATIONAL INFO SERV (FIS)\", \"FIS\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FISERV INC\", \"FI\"),\n",
    "]\n",
    "map_df = pd.DataFrame(stock_mapping, columns=[\"Sub Theme\", \"Company\", \"Ticker\"])\n",
    "\n",
    "\n",
    "\n",
    "def download_prices_chunked(tickers, start, end, interval=\"1d\", pause=1.2):\n",
    "    rows = []\n",
    "    ok, bad = [], []\n",
    "    for t in tickers:\n",
    "        try:\n",
    "            df = yf.download(\n",
    "                t, start=start, end=end, interval=interval,\n",
    "                auto_adjust=False, progress=False, threads=False\n",
    "            )\n",
    "            if df.empty:\n",
    "                bad.append((t, \"empty\"))\n",
    "            else:\n",
    "                df = df.reset_index().rename(columns={\"Adj Close\": \"AdjClose\"})\n",
    "                df[\"Ticker\"] = t\n",
    "                rows.append(df[[\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"AdjClose\",\"Volume\"]])\n",
    "                ok.append(t)\n",
    "        except Exception as e:\n",
    "            bad.append((t, str(e)))\n",
    "        time.sleep(pause)  # é™æµå‹å¥½ / be nice to rate limits\n",
    "    if rows:\n",
    "        out = pd.concat(rows, ignore_index=True)\n",
    "    else:\n",
    "        out = pd.DataFrame(columns=[\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"AdjClose\",\"Volume\"])\n",
    "    return out, ok, bad\n",
    "\n",
    "tickers = map_df[\"Ticker\"].unique().tolist()\n",
    "prices_long, ok, bad = download_prices_chunked(\n",
    "    tickers, start=\"2020-01-01\", end=\"2025-10-01\", interval=\"1d\", pause=1.2\n",
    ")\n",
    "\n",
    "print(\"Succeeded:\", ok)\n",
    "print(\"Failed:\", bad)\n",
    "\n",
    "# åˆä½µæ˜ å°„ / merge mapping\n",
    "df = prices_long.merge(map_df, on=\"Ticker\", how=\"left\")\n",
    "\n",
    "# è¨ˆç®—å°æ•¸æ”¶ç›Š / log returns\n",
    "df = df.sort_values([\"Ticker\",\"Date\"])\n",
    "df[\"r_t\"] = df.groupby(\"Ticker\")[\"AdjClose\"].transform(lambda s: np.log(s / s.shift(1)))\n",
    "# å¦‚æœä½ æƒ³é¿å…ç¬¬ä¸€è¡Œ NaNï¼šå¡« 0\n",
    "# df[\"r_t\"] = df[\"r_t\"].fillna(0)\n",
    "\n",
    "# æ•´ç†åˆ—é †åº / reorder\n",
    "cols = [\"Date\",\"Sub Theme\",\"Company\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"AdjClose\",\"Volume\",\"r_t\"]\n",
    "df = df[[c for c in cols if c in df.columns]]\n",
    "\n",
    "df.to_csv(\"fintech_prices.csv\", index=False)\n",
    "print(df.head())\n"
   ],
   "id": "a0917496a2e1368b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['COIN']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['XYZ']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['RIOT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['CIFR']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['MARA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['AFRM']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['APTY']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['MQ']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['BFH']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['SEZL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['TOST']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['FOUR']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['VERI']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['PAX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['INTU']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['SSNC']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['BL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['FIS']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "\n",
      "1 Failed download:\n",
      "['FI']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded: []\n",
      "Failed: [('COIN', 'empty'), ('XYZ', 'empty'), ('RIOT', 'empty'), ('CIFR', 'empty'), ('MARA', 'empty'), ('AFRM', 'empty'), ('APTY', 'empty'), ('MQ', 'empty'), ('BFH', 'empty'), ('SEZL', 'empty'), ('TOST', 'empty'), ('FOUR', 'empty'), ('VERI', 'empty'), ('PAX', 'empty'), ('INTU', 'empty'), ('SSNC', 'empty'), ('BL', 'empty'), ('FIS', 'empty'), ('FI', 'empty')]\n",
      "Empty DataFrame\n",
      "Columns: [Date, Sub Theme, Company, Ticker, Open, High, Low, Close, AdjClose, Volume, r_t]\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ---- 1) æ˜ å°„è¡¨ / Mapping table ----\n",
    "rows = [\n",
    "    # SubTheme, Company, Ticker\n",
    "    (\"Cryptocurrency\", \"COINBASE GLOBAL INC -CLASS A\", \"COIN\"),\n",
    "    (\"Cryptocurrency\", \"BLOCK INC\", \"XYZ\"),   # ç”¨ä½ è¡¨é‡Œçš„tickerï¼›è‹¥å®é™…ä¸‹è½½å¤±è´¥ï¼Œå¯æŠŠå®ƒæ”¹æˆ SQ\n",
    "    (\"Cryptocurrency\", \"RIOT PLATFORMS INC\", \"RIOT\"),\n",
    "    (\"Cryptocurrency\", \"CIPHER MINING INC\", \"CIFR\"),\n",
    "    (\"Cryptocurrency\", \"MARATHON HOLDINGS INC\", \"MARA\"),\n",
    "    (\"Credit Card & BNPL\", \"AFFIRM HOLDINGS INC\", \"AFRM\"),\n",
    "    (\"Credit Card & BNPL\", \"AFTERPAY\", \"APTY\"),\n",
    "    (\"Credit Card & BNPL\", \"MARQETA INC-A\", \"MQ\"),\n",
    "    (\"Credit Card & BNPL\", \"BREAD FINANCIAL HOLDINGS INC\", \"BFH\"),\n",
    "    (\"Credit Card & BNPL\", \"SEZZLE INC\", \"SEZL\"),\n",
    "    (\"Fintech Hardware\", \"TOAST INC-CLASS A\", \"TOST\"),\n",
    "    (\"Fintech Hardware\", \"SHIFT4 PAYMENTS INC\", \"FOUR\"),\n",
    "    (\"Fintech Hardware\", \"VERIFONE\", \"VERI\"),\n",
    "    (\"Fintech Hardware\", \"PAX GLOBAL TECHNOLOGY LTD\", \"PAX\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"INTUIT INC\", \"INTU\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"SS&C TECHNOLOGIES HOLDINGS\", \"SSNC\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"BLACKLINE INC\", \"BL\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FIDELITY NATIONAL INFO SERV (FIS)\", \"FIS\"),\n",
    "    (\"Data Processing & Outsourced Services\", \"FISERV INC\", \"FI\"),\n",
    "]\n",
    "\n",
    "map_df = pd.DataFrame(rows, columns=[\"Sub Theme\", \"Company\", \"Ticker\"])\n",
    "\n",
    "print(f\"å°†ä¸‹è½½ {len(map_df)} åªè‚¡ç¥¨çš„æ•°æ®\")\n",
    "\n",
    "# ---- 2) ä¸‹è½½ä»·æ ¼ / Download prices ----\n",
    "tickers = map_df[\"Ticker\"].unique().tolist()\n",
    "\n",
    "try:\n",
    "    prices = yf.download(\n",
    "        tickers, start=\"2020-01-01\", end=\"2025-10-01\",\n",
    "        interval=\"1d\", group_by=\"ticker\", auto_adjust=True, threads=True\n",
    "    )\n",
    "\n",
    "    # æ£€æŸ¥ä¸‹è½½ç»“æœ\n",
    "    if prices.empty:\n",
    "        raise ValueError(\"æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ•°æ®\")\n",
    "\n",
    "    # å¤„ç†å•åªè‚¡ç¥¨çš„æƒ…å†µ\n",
    "    if len(tickers) == 1 and not isinstance(prices.columns, pd.MultiIndex):\n",
    "        prices = pd.DataFrame(prices)\n",
    "        prices.columns = pd.MultiIndex.from_product([prices.columns, [tickers[0]]])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ä¸‹è½½ä»·æ ¼æ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- 3) è½¬æ¢æ•°æ®æ ¼å¼ / Transform to long format ----\n",
    "df = prices.stack(level=0).reset_index()\n",
    "df = df.rename(columns={\"level_0\": \"Date\", \"level_1\": \"Ticker\"})\n",
    "\n",
    "# ---- 4) åˆå¹¶æ˜ å°„ / Merge mapping ----\n",
    "df = df.merge(map_df, on=\"Ticker\", how=\"left\")\n",
    "\n",
    "# ---- 5) è®¡ç®—åŸºç¡€æŒ‡æ ‡ / Compute basic metrics ----\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "# ç¡®ä¿åˆ—åæ­£ç¡®\n",
    "if 'Adj Close' in df.columns:\n",
    "    price_col = 'Adj Close'\n",
    "elif 'Close' in df.columns:\n",
    "    price_col = 'Close'\n",
    "    df['Adj Close'] = df['Close']  # å¦‚æœæ²¡æœ‰è°ƒæ•´åæ”¶ç›˜ä»·ï¼Œä½¿ç”¨æ”¶ç›˜ä»·\n",
    "else:\n",
    "    raise ValueError(\"æ²¡æœ‰æ‰¾åˆ°ä»·æ ¼åˆ—\")\n",
    "\n",
    "# è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡\n",
    "df[\"r_t\"] = df.groupby(\"Ticker\")[price_col].transform(lambda s: np.log(s / s.shift(1)))\n",
    "\n",
    "# ---- 6) æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•° / Technical indicator functions ----\n",
    "def calculate_technical_indicators(df_group):\n",
    "    \"\"\"ä¸ºå•ä¸ªè‚¡ç¥¨è®¡ç®—æŠ€æœ¯æŒ‡æ ‡\"\"\"\n",
    "    group = df_group.copy().sort_values('Date')\n",
    "\n",
    "    # ä½¿ç”¨è°ƒæ•´åæ”¶ç›˜ä»·æˆ–æ”¶ç›˜ä»·\n",
    "    close = group[price_col].values\n",
    "\n",
    "    # === ç§»åŠ¨å¹³å‡çº¿ ===\n",
    "    group['MA_5'] = group[price_col].rolling(window=5).mean()\n",
    "    group['MA_20'] = group[price_col].rolling(window=20).mean()\n",
    "    group['MA_50'] = group[price_col].rolling(window=50).mean()\n",
    "    group['MA_200'] = group[price_col].rolling(window=200).mean()\n",
    "\n",
    "    # === æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ ===\n",
    "    group['EMA_12'] = group[price_col].ewm(span=12).mean()\n",
    "    group['EMA_26'] = group[price_col].ewm(span=26).mean()\n",
    "\n",
    "    # === MACD ===\n",
    "    group['MACD'] = group['EMA_12'] - group['EMA_26']\n",
    "    group['MACD_Signal'] = group['MACD'].ewm(span=9).mean()\n",
    "    group['MACD_Histogram'] = group['MACD'] - group['MACD_Signal']\n",
    "\n",
    "    # === RSI ===\n",
    "    def compute_rsi(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    group['RSI_14'] = compute_rsi(group[price_col])\n",
    "\n",
    "    # === å¸ƒæ—å¸¦ ===\n",
    "    rolling_mean = group[price_col].rolling(window=20).mean()\n",
    "    rolling_std = group[price_col].rolling(window=20).std()\n",
    "    group['Bollinger_Upper'] = rolling_mean + (rolling_std * 2)\n",
    "    group['Bollinger_Lower'] = rolling_mean - (rolling_std * 2)\n",
    "    group['Bollinger_Middle'] = rolling_mean\n",
    "    group['Bollinger_Width'] = (group['Bollinger_Upper'] - group['Bollinger_Lower']) / group['Bollinger_Middle']\n",
    "\n",
    "    # === æ³¢åŠ¨ç‡æŒ‡æ ‡ ===\n",
    "    group['Volatility_20d'] = group['r_t'].rolling(window=20).std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡\n",
    "    group['Volatility_60d'] = group['r_t'].rolling(window=60).std() * np.sqrt(252)\n",
    "\n",
    "    # çœŸå®æ³¢åŠ¨å¹…åº¦ (ATR)\n",
    "    high_low = group['High'] - group['Low']\n",
    "    high_close = np.abs(group['High'] - group[price_col].shift())\n",
    "    low_close = np.abs(group['Low'] - group[price_col].shift())\n",
    "    true_range = np.maximum(np.maximum(high_low, high_close), low_close)\n",
    "    group['ATR_14'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    # === åŠ¨é‡æŒ‡æ ‡ ===\n",
    "    group['Momentum_1M'] = group[price_col] / group[price_col].shift(21) - 1  # 1ä¸ªæœˆåŠ¨é‡\n",
    "    group['Momentum_3M'] = group[price_col] / group[price_col].shift(63) - 1  # 3ä¸ªæœˆåŠ¨é‡\n",
    "    group['Momentum_6M'] = group[price_col] / group[price_col].shift(126) - 1  # 6ä¸ªæœˆåŠ¨é‡\n",
    "\n",
    "    # === æ”¯æ’‘é˜»åŠ› ===\n",
    "    group['Support_Resistance_Ratio'] = (group[price_col] - group['Bollinger_Lower']) / (group['Bollinger_Upper'] - group['Bollinger_Lower'])\n",
    "\n",
    "    # === ä»·æ ¼ä½ç½® ===\n",
    "    rolling_min = group[price_col].rolling(window=20).min()\n",
    "    rolling_max = group[price_col].rolling(window=20).max()\n",
    "    group['Price_Position_20d'] = (group[price_col] - rolling_min) / (rolling_max - rolling_min)\n",
    "\n",
    "    return group\n",
    "\n",
    "# ---- 7) åº”ç”¨æŠ€æœ¯æŒ‡æ ‡è®¡ç®— / Apply technical indicators ----\n",
    "print(\"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...\")\n",
    "df_with_indicators = df.groupby('Ticker').apply(calculate_technical_indicators).reset_index(drop=True)\n",
    "\n",
    "# ---- 8) åˆ›å»ºåˆ†ææŒ‡æ ‡æ±‡æ€»DataFrame / Create analysis summary DataFrame ----\n",
    "def create_analysis_summary(df_with_indicators):\n",
    "    \"\"\"åˆ›å»ºæ¯ä¸ªè‚¡ç¥¨çš„æŒ‡æ ‡æ±‡æ€»è¡¨\"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    for ticker in df_with_indicators['Ticker'].unique():\n",
    "        ticker_data = df_with_indicators[df_with_indicators['Ticker'] == ticker].copy()\n",
    "\n",
    "        if ticker_data.empty:\n",
    "            continue\n",
    "\n",
    "        # è·å–æœ€æ–°æ•°æ®\n",
    "        latest = ticker_data.iloc[-1]\n",
    "\n",
    "        # è·å–å­ä¸»é¢˜å’Œå…¬å¸åç§°\n",
    "        sub_theme = latest['Sub Theme']\n",
    "        company = latest['Company']\n",
    "\n",
    "        # åŸºç¡€ä»·æ ¼ä¿¡æ¯\n",
    "        current_price = latest[price_col]\n",
    "        price_1d_change = ticker_data['r_t'].iloc[-1] if not pd.isna(ticker_data['r_t'].iloc[-1]) else 0\n",
    "\n",
    "        # è®¡ç®—å„ç§æ—¶é—´å‘¨æœŸçš„æ”¶ç›Šç‡\n",
    "        periods = {\n",
    "            '1W': 5, '1M': 21, '3M': 63, '6M': 126, '1Y': 252, 'YTD': 'ytd'\n",
    "        }\n",
    "\n",
    "        returns = {}\n",
    "        for period_name, period_days in periods.items():\n",
    "            if period_days == 'ytd':\n",
    "                # å¹´åˆè‡³ä»Šæ”¶ç›Š\n",
    "                ytd_data = ticker_data[ticker_data['Date'] >= f\"{datetime.now().year}-01-01\"]\n",
    "                if len(ytd_data) > 1:\n",
    "                    returns[period_name] = (ytd_data[price_col].iloc[-1] / ytd_data[price_col].iloc[0] - 1) * 100\n",
    "                else:\n",
    "                    returns[period_name] = 0\n",
    "            else:\n",
    "                if len(ticker_data) > period_days:\n",
    "                    returns[period_name] = (ticker_data[price_col].iloc[-1] / ticker_data[price_col].iloc[-period_days-1] - 1) * 100\n",
    "                else:\n",
    "                    returns[period_name] = 0\n",
    "\n",
    "        # æŠ€æœ¯æŒ‡æ ‡çŠ¶æ€\n",
    "        ma_trend = \"ä¸Šæ¶¨\" if latest['MA_20'] > latest['MA_50'] else \"ä¸‹è·Œ\" if latest['MA_20'] < latest['MA_50'] else \"ç›˜æ•´\"\n",
    "\n",
    "        rsi_status = \"è¶…ä¹°\" if latest.get('RSI_14', 50) > 70 else \"è¶…å–\" if latest.get('RSI_14', 50) < 30 else \"ä¸­æ€§\"\n",
    "\n",
    "        macd_signal = \"ä¹°å…¥\" if latest.get('MACD_Histogram', 0) > 0 else \"å–å‡º\" if latest.get('MACD_Histogram', 0) < 0 else \"ä¸­æ€§\"\n",
    "\n",
    "        volatility_level = \"é«˜\" if latest.get('Volatility_20d', 0) > 0.4 else \"ä½\" if latest.get('Volatility_20d', 0) < 0.2 else \"ä¸­\"\n",
    "\n",
    "        # æ”¯æ’‘é˜»åŠ›çŠ¶æ€\n",
    "        if latest.get('Support_Resistance_Ratio', 0.5) > 0.7:\n",
    "            sr_status = \"æ¥è¿‘é˜»åŠ›\"\n",
    "        elif latest.get('Support_Resistance_Ratio', 0.5) < 0.3:\n",
    "            sr_status = \"æ¥è¿‘æ”¯æ’‘\"\n",
    "        else:\n",
    "            sr_status = \"ä¸­é—´åŒºåŸŸ\"\n",
    "\n",
    "        summary_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Company': company,\n",
    "            'Sub Theme': sub_theme,\n",
    "            'Current Price': current_price,\n",
    "            '1D Return (%)': price_1d_change * 100,\n",
    "            **returns,\n",
    "            'MA Trend': ma_trend,\n",
    "            'RSI Status': rsi_status,\n",
    "            'MACD Signal': macd_signal,\n",
    "            'Volatility Level': volatility_level,\n",
    "            'Support Resistance': sr_status,\n",
    "            '20D Volatility': latest.get('Volatility_20d', 0),\n",
    "            'RSI': latest.get('RSI_14', 50),\n",
    "            'Volume (1M avg)': ticker_data['Volume'].tail(21).mean(),\n",
    "            'Last Updated': latest['Date']\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# ---- 9) ç”Ÿæˆåˆ†ææ±‡æ€»è¡¨ / Generate analysis summary ----\n",
    "print(\"ç”Ÿæˆåˆ†ææ±‡æ€»è¡¨...\")\n",
    "analysis_summary = create_analysis_summary(df_with_indicators)\n",
    "\n",
    "# ---- 10) æ•°æ®è¾“å‡º / Data output ----\n",
    "# åŸå§‹æ•°æ®ï¼ˆåŒ…å«æŠ€æœ¯æŒ‡æ ‡ï¼‰\n",
    "output_cols = [\"Date\", \"Sub Theme\", \"Company\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\",\n",
    "               \"Adj Close\", \"Volume\", \"r_t\", \"MA_5\", \"MA_20\", \"MA_50\", \"MA_200\",\n",
    "               \"RSI_14\", \"MACD\", \"MACD_Signal\", \"MACD_Histogram\", \"Volatility_20d\"]\n",
    "\n",
    "# åªé€‰æ‹©å­˜åœ¨çš„åˆ—\n",
    "available_cols = [col for col in output_cols if col in df_with_indicators.columns]\n",
    "df_final = df_with_indicators[available_cols]\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "df_final.to_csv('stock_data_with_indicators.csv', index=False)\n",
    "analysis_summary.to_csv('stock_analysis_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\n=== æ•°æ®ç”Ÿæˆå®Œæˆ ===\")\n",
    "print(f\"åŸå§‹æ•°æ®ï¼ˆå«æŠ€æœ¯æŒ‡æ ‡ï¼‰: {len(df_final)} è¡Œï¼Œä¿å­˜è‡³ stock_data_with_indicators.csv\")\n",
    "print(f\"åˆ†ææ±‡æ€»è¡¨: {len(analysis_summary)} åªè‚¡ç¥¨ï¼Œä¿å­˜è‡³ stock_analysis_summary.csv\")\n",
    "\n",
    "print(f\"\\n=== åˆ†ææ±‡æ€»è¡¨å‰10è¡Œ ===\")\n",
    "print(analysis_summary.head(10))\n",
    "\n",
    "print(f\"\\n=== å…³é”®æŒ‡æ ‡ç»Ÿè®¡ ===\")\n",
    "print(f\"å¹³å‡20æ—¥æ³¢åŠ¨ç‡: {analysis_summary['20D Volatility'].mean():.2%}\")\n",
    "print(f\"RSIä¸­æ€§æ¯”ä¾‹: {(analysis_summary['RSI'].between(30, 70)).mean():.1%}\")\n",
    "print(f\"ä¸Šæ¶¨è¶‹åŠ¿è‚¡ç¥¨: {(analysis_summary['MA Trend'] == 'ä¸Šæ¶¨').mean():.1%}\")\n",
    "\n",
    "# ---- 11) é¢å¤–åˆ†æï¼šæŒ‰å­ä¸»é¢˜åˆ†ç»„ç»Ÿè®¡ ----\n",
    "if 'Sub Theme' in analysis_summary.columns:\n",
    "    print(f\"\\n=== æŒ‰å­ä¸»é¢˜åˆ†æ ===\")\n",
    "    theme_analysis = analysis_summary.groupby('Sub Theme').agg({\n",
    "        '1D Return (%)': 'mean',\n",
    "        '1M Return (%)': 'mean',\n",
    "        '20D Volatility': 'mean',\n",
    "        'RSI': 'mean',\n",
    "        'Ticker': 'count'\n",
    "    }).round(3)\n",
    "\n",
    "    theme_analysis = theme_analysis.rename(columns={'Ticker': 'Stock Count'})\n",
    "    print(theme_analysis)"
   ],
   "id": "126614ce851a5be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T15:44:04.916748Z",
     "start_time": "2025-10-02T15:44:02.607604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yfinance as yf\n",
    "\n",
    "ticker_list = [\"AAPL\",\"MSFT\"]\n",
    "\n",
    "for t in ticker_list:\n",
    "    tk = yf.Ticker(t)\n",
    "\n",
    "    # åŸºæœ¬è´¢åŠ¡æŠ¥è¡¨ / Financial Statements\n",
    "    print(f\"=== {t} Balance Sheet ===\")\n",
    "    print(tk.balance_sheet.head())\n",
    "\n",
    "    print(f\"=== {t} Income Statement ===\")\n",
    "    print(tk.financials.head())\n",
    "\n",
    "    print(f\"=== {t} Cash Flow ===\")\n",
    "    print(tk.cashflow.head())\n",
    "\n",
    "    # å…¬å¸æ¦‚å†µ / Company Info\n",
    "    print(tk.info)  # dictï¼ŒåŒ…æ‹¬å¸‚å€¼ã€å¸‚ç›ˆç‡ã€è¡Œä¸šã€Beta ç­‰\n",
    "\n",
    "    # åˆ†æå¸ˆæŒ‡æ ‡ / Analysts\n",
    "    print(tk.analyst_price_targets)\n",
    "    print(tk.recommendations.tail())\n"
   ],
   "id": "f88d31f1c2d4b166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AAPL Balance Sheet ===\n",
      "                          2024-09-30    2023-09-30    2022-09-30    2021-09-30\n",
      "Treasury Shares Number           NaN  0.000000e+00           NaN           NaN\n",
      "Ordinary Shares Number  1.511679e+10  1.555006e+10  1.594342e+10  1.642679e+10\n",
      "Share Issued            1.511679e+10  1.555006e+10  1.594342e+10  1.642679e+10\n",
      "Net Debt                7.668600e+10  8.112300e+10  9.642300e+10  8.977900e+10\n",
      "Total Debt              1.066290e+11  1.110880e+11  1.324800e+11  1.365220e+11\n",
      "=== AAPL Income Statement ===\n",
      "                                                      2024-09-30  \\\n",
      "Tax Effect Of Unusual Items                         0.000000e+00   \n",
      "Tax Rate For Calcs                                  2.410000e-01   \n",
      "Normalized EBITDA                                   1.346610e+11   \n",
      "Net Income From Continuing Operation Net Minori...  9.373600e+10   \n",
      "Reconciled Depreciation                             1.144500e+10   \n",
      "\n",
      "                                                      2023-09-30  \\\n",
      "Tax Effect Of Unusual Items                         0.000000e+00   \n",
      "Tax Rate For Calcs                                  1.470000e-01   \n",
      "Normalized EBITDA                                   1.258200e+11   \n",
      "Net Income From Continuing Operation Net Minori...  9.699500e+10   \n",
      "Reconciled Depreciation                             1.151900e+10   \n",
      "\n",
      "                                                      2022-09-30    2021-09-30  \n",
      "Tax Effect Of Unusual Items                         0.000000e+00  0.000000e+00  \n",
      "Tax Rate For Calcs                                  1.620000e-01  1.330000e-01  \n",
      "Normalized EBITDA                                   1.305410e+11  1.231360e+11  \n",
      "Net Income From Continuing Operation Net Minori...  9.980300e+10  9.468000e+10  \n",
      "Reconciled Depreciation                             1.110400e+10  1.128400e+10  \n",
      "=== AAPL Cash Flow ===\n",
      "                               2024-09-30    2023-09-30    2022-09-30  \\\n",
      "Free Cash Flow               1.088070e+11  9.958400e+10  1.114430e+11   \n",
      "Repurchase Of Capital Stock -9.494900e+10 -7.755000e+10 -8.940200e+10   \n",
      "Repayment Of Debt           -9.958000e+09 -1.115100e+10 -9.543000e+09   \n",
      "Issuance Of Debt             0.000000e+00  5.228000e+09  5.465000e+09   \n",
      "Issuance Of Capital Stock             NaN           NaN           NaN   \n",
      "\n",
      "                               2021-09-30  \n",
      "Free Cash Flow               9.295300e+10  \n",
      "Repurchase Of Capital Stock -8.597100e+10  \n",
      "Repayment Of Debt           -8.750000e+09  \n",
      "Issuance Of Debt             2.039300e+10  \n",
      "Issuance Of Capital Stock    1.105000e+09  \n",
      "{'address1': 'One Apple Park Way', 'city': 'Cupertino', 'state': 'CA', 'zip': '95014', 'country': 'United States', 'phone': '(408) 996-1010', 'website': 'https://www.apple.com', 'industry': 'Consumer Electronics', 'industryKey': 'consumer-electronics', 'industryDisp': 'Consumer Electronics', 'sector': 'Technology', 'sectorKey': 'technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Apple Inc. designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide. The company offers iPhone, a line of smartphones; Mac, a line of personal computers; iPad, a line of multi-purpose tablets; and wearables, home, and accessories comprising AirPods, Apple TV, Apple Watch, Beats products, and HomePod. It also provides AppleCare support and cloud services; and operates various platforms, including the App Store that allow customers to discover and download applications and digital content, such as books, music, video, games, and podcasts, as well as advertising services include third-party licensing arrangements and its own advertising platforms. In addition, the company offers various subscription-based services, such as Apple Arcade, a game subscription service; Apple Fitness+, a personalized fitness service; Apple Music, which offers users a curated listening experience with on-demand radio stations; Apple News+, a subscription news and magazine service; Apple TV+, which offers exclusive original content; Apple Card, a co-branded credit card; and Apple Pay, a cashless payment service, as well as licenses its intellectual property. The company serves consumers, and small and mid-sized businesses; and the education, enterprise, and government markets. It distributes third-party applications for its products through the App Store. The company also sells its products through its retail and online stores, and direct sales force; and third-party cellular network carriers, wholesalers, retailers, and resellers. Apple Inc. was founded in 1976 and is headquartered in Cupertino, California.', 'fullTimeEmployees': 150000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Timothy D. Cook', 'age': 63, 'title': 'CEO & Director', 'yearBorn': 1961, 'fiscalYear': 2024, 'totalPay': 16520856, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Jeffrey E. Williams', 'age': 60, 'title': 'Senior Vice President of Design, Watch, & Health', 'yearBorn': 1964, 'fiscalYear': 2024, 'totalPay': 5020737, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Kevan  Parekh', 'age': 52, 'title': 'Senior VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Sabih  Khan', 'title': 'Chief Operating Officer', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Chris  Kondo', 'title': 'Senior Director of Corporate Accounting', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Suhasini  Chandramouli', 'title': 'Director of Investor Relations', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Kristin Huguet Quayle', 'title': 'Vice President of Worldwide Communications', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Greg  Joswiak', 'title': 'Senior Vice President of Worldwide Marketing', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Adrian  Perica', 'age': 50, 'title': 'Vice President of Corporate Development', 'yearBorn': 1974, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Michael  Fenger', 'title': 'VP of Worldwide Sales', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 7, 'boardRisk': 1, 'compensationRisk': 3, 'shareHolderRightsRisk': 1, 'overallRisk': 1, 'governanceEpochDate': 1759276800, 'compensationAsOfEpochDate': 1735603200, 'irWebsite': 'http://investor.apple.com/', 'executiveTeam': [], 'maxAge': 86400, 'priceHint': 2, 'previousClose': 255.45, 'open': 256.59, 'dayLow': 254.15, 'dayHigh': 257.6703, 'regularMarketPreviousClose': 255.45, 'regularMarketOpen': 256.59, 'regularMarketDayLow': 254.15, 'regularMarketDayHigh': 257.6703, 'dividendRate': 1.04, 'dividendYield': 0.41, 'exDividendDate': 1754870400, 'payoutRatio': 0.1533, 'fiveYearAvgDividendYield': 0.54, 'beta': 1.094, 'trailingPE': 38.900074, 'forwardPE': 30.988987, 'volume': 17812842, 'regularMarketVolume': 17812842, 'averageVolume': 55007147, 'averageVolume10days': 64382750, 'averageDailyVolume10Day': 64382750, 'bid': 255.03, 'ask': 257.57, 'bidSize': 4, 'askSize': 2, 'marketCap': 3821674692608, 'fiftyTwoWeekLow': 169.21, 'fiftyTwoWeekHigh': 260.1, 'allTimeHigh': 260.1, 'allTimeLow': 0.049107, 'priceToSalesTrailing12Months': 9.352523, 'fiftyDayAverage': 230.9796, 'twoHundredDayAverage': 222.0911, 'trailingAnnualDividendRate': 1.01, 'trailingAnnualDividendYield': 0.003953807, 'currency': 'USD', 'tradeable': False, 'enterpriseValue': 3837303717888, 'profitMargins': 0.24295999, 'floatShares': 14814419318, 'sharesOutstanding': 14840390000, 'sharesShort': 118199377, 'sharesShortPriorMonth': 127079143, 'sharesShortPreviousMonthDate': 1755216000, 'dateShortInterest': 1757894400, 'sharesPercentSharesOut': 0.008, 'heldPercentInsiders': 0.0197, 'heldPercentInstitutions': 0.63624, 'shortRatio': 2.47, 'shortPercentOfFloat': 0.008, 'impliedSharesOutstanding': 14840970951, 'bookValue': 4.431, 'priceToBook': 58.117462, 'lastFiscalYearEnd': 1727481600, 'nextFiscalYearEnd': 1759017600, 'mostRecentQuarter': 1751068800, 'earningsQuarterlyGrowth': 0.093, 'netIncomeToCommon': 99280003072, 'trailingEps': 6.62, 'forwardEps': 8.31, 'lastSplitFactor': '4:1', 'lastSplitDate': 1598832000, 'enterpriseToRevenue': 9.391, 'enterpriseToEbitda': 27.081, '52WeekChange': 0.13196266, 'SandP52WeekChange': 0.17741585, 'lastDividendValue': 0.26, 'lastDividendDate': 1754870400, 'quoteType': 'EQUITY', 'currentPrice': 257.5185, 'targetHighPrice': 310.0, 'targetLowPrice': 175.0, 'targetMeanPrice': 246.2347, 'targetMedianPrice': 245.0, 'recommendationMean': 2.0, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 41, 'totalCash': 55372001280, 'totalCashPerShare': 3.731, 'ebitda': 141696008192, 'totalDebt': 101698002944, 'quickRatio': 0.724, 'currentRatio': 0.868, 'totalRevenue': 408624988160, 'debtToEquity': 154.486, 'revenuePerShare': 27.173, 'returnOnAssets': 0.24545999, 'returnOnEquity': 1.49814, 'grossProfits': 190739005440, 'freeCashflow': 94873747456, 'operatingCashflow': 108564996096, 'earningsGrowth': 0.121, 'revenueGrowth': 0.096, 'grossMargins': 0.46678, 'ebitdaMargins': 0.34675997, 'operatingMargins': 0.29990998, 'financialCurrency': 'USD', 'symbol': 'AAPL', 'language': 'en-US', 'region': 'US', 'typeDisp': 'Equity', 'quoteSourceName': 'Nasdaq Real Time Price', 'triggerable': True, 'customPriceAlertConfidence': 'HIGH', 'corporateActions': [], 'regularMarketTime': 1759419841, 'regularMarketChangePercent': 0.8097462, 'regularMarketPrice': 257.5185, 'longName': 'Apple Inc.', 'shortName': 'Apple Inc.', 'marketState': 'REGULAR', 'messageBoardId': 'finmb_24937', 'exchangeTimezoneName': 'America/New_York', 'exchangeTimezoneShortName': 'EDT', 'gmtOffSetMilliseconds': -14400000, 'market': 'us_market', 'esgPopulated': False, 'regularMarketChange': 2.0684967, 'regularMarketDayRange': '254.15 - 257.6703', 'fullExchangeName': 'NasdaqGS', 'averageDailyVolume3Month': 55007147, 'exchange': 'NMS', 'hasPrePostMarketData': True, 'firstTradeDateMilliseconds': 345479400000, 'fiftyTwoWeekLowChange': 88.30849, 'fiftyTwoWeekLowChangePercent': 0.5218869, 'fiftyTwoWeekRange': '169.21 - 260.1', 'fiftyTwoWeekHighChange': -2.5815125, 'fiftyTwoWeekHighChangePercent': -0.009925077, 'fiftyTwoWeekChangePercent': 13.196266, 'dividendDate': 1755129600, 'earningsTimestamp': 1753992000, 'earningsTimestampStart': 1761854400, 'earningsTimestampEnd': 1761854400, 'earningsCallTimestampStart': 1753995600, 'earningsCallTimestampEnd': 1753995600, 'isEarningsDateEstimate': True, 'epsTrailingTwelveMonths': 6.62, 'epsForward': 8.31, 'epsCurrentYear': 7.39388, 'priceEpsCurrentYear': 34.8286, 'fiftyDayAverageChange': 26.538895, 'fiftyDayAverageChangePercent': 0.11489714, 'twoHundredDayAverageChange': 35.4274, 'twoHundredDayAverageChangePercent': 0.15951742, 'sourceInterval': 15, 'exchangeDataDelayedBy': 0, 'averageAnalystRating': '2.0 - Buy', 'cryptoTradeable': False, 'displayName': 'Apple', 'trailingPegRatio': 2.4377}\n",
      "{'current': 257.5185, 'high': 310.0, 'low': 175.0, 'mean': 246.2347, 'median': 245.0}\n",
      "  period  strongBuy  buy  hold  sell  strongSell\n",
      "0     0m          5   24    15     1           3\n",
      "1    -1m          5   23    15     1           3\n",
      "2    -2m          5   22    15     1           1\n",
      "3    -3m          5   23    15     1           1\n",
      "=== MSFT Balance Sheet ===\n",
      "                          2025-06-30    2024-06-30    2023-06-30  \\\n",
      "Ordinary Shares Number  7.434000e+09  7.434139e+09  7.432000e+09   \n",
      "Share Issued            7.434000e+09  7.434139e+09  7.432000e+09   \n",
      "Net Debt                1.290900e+10  3.331500e+10  1.253300e+10   \n",
      "Total Debt              6.058800e+10  6.712700e+10  5.996500e+10   \n",
      "Tangible Book Value     2.013660e+11  1.216600e+11  1.289710e+11   \n",
      "\n",
      "                          2022-06-30  2021-06-30  \n",
      "Ordinary Shares Number  7.464000e+09         NaN  \n",
      "Share Issued            7.464000e+09         NaN  \n",
      "Net Debt                3.585000e+10         NaN  \n",
      "Total Debt              6.127000e+10         NaN  \n",
      "Tangible Book Value     8.772000e+10         NaN  \n",
      "=== MSFT Income Statement ===\n",
      "                                          2025-06-30    2024-06-30  \\\n",
      "Tax Effect Of Unusual Items            -7.708800e+07 -9.991800e+07   \n",
      "Tax Rate For Calcs                      1.760000e-01  1.820000e-01   \n",
      "Normalized EBITDA                       1.606030e+11  1.335580e+11   \n",
      "Total Unusual Items                    -4.380000e+08 -5.490000e+08   \n",
      "Total Unusual Items Excluding Goodwill -4.380000e+08 -5.490000e+08   \n",
      "\n",
      "                                          2023-06-30    2022-06-30  \n",
      "Tax Effect Of Unusual Items            -2.850000e+06  4.375400e+07  \n",
      "Tax Rate For Calcs                      1.900000e-01  1.310000e-01  \n",
      "Normalized EBITDA                       1.051550e+11  9.990500e+10  \n",
      "Total Unusual Items                    -1.500000e+07  3.340000e+08  \n",
      "Total Unusual Items Excluding Goodwill -1.500000e+07  3.340000e+08  \n",
      "=== MSFT Cash Flow ===\n",
      "                               2025-06-30    2024-06-30    2023-06-30  \\\n",
      "Free Cash Flow               7.161100e+10  7.407100e+10  5.947500e+10   \n",
      "Repurchase Of Capital Stock -1.842000e+10 -1.725400e+10 -2.224500e+10   \n",
      "Repayment Of Debt           -3.216000e+09 -2.907000e+10 -2.750000e+09   \n",
      "Issuance Of Debt             0.000000e+00  2.439500e+10  0.000000e+00   \n",
      "Issuance Of Capital Stock    2.056000e+09  2.002000e+09  1.866000e+09   \n",
      "\n",
      "                               2022-06-30  2021-06-30  \n",
      "Free Cash Flow               6.514900e+10         NaN  \n",
      "Repurchase Of Capital Stock -3.269600e+10         NaN  \n",
      "Repayment Of Debt           -9.023000e+09         NaN  \n",
      "Issuance Of Debt             0.000000e+00         NaN  \n",
      "Issuance Of Capital Stock    1.841000e+09         NaN  \n",
      "{'address1': 'One Microsoft Way', 'city': 'Redmond', 'state': 'WA', 'zip': '98052-6399', 'country': 'United States', 'phone': '425 882 8080', 'website': 'https://www.microsoft.com', 'industry': 'Software - Infrastructure', 'industryKey': 'software-infrastructure', 'industryDisp': 'Software - Infrastructure', 'sector': 'Technology', 'sectorKey': 'technology', 'sectorDisp': 'Technology', 'longBusinessSummary': \"Microsoft Corporation develops and supports software, services, devices, and solutions worldwide. The company's Productivity and Business Processes segment offers Microsoft 365 Commercial, Enterprise Mobility + Security, Windows Commercial, Power BI, Exchange, SharePoint, Microsoft Teams, Security and Compliance, and Copilot; Microsoft 365 Commercial products, such as Windows Commercial on-premises and Office licensed services; Microsoft 365 Consumer products and cloud services, such as Microsoft 365 Consumer subscriptions, Office licensed on-premises, and other consumer services; LinkedIn; Dynamics products and cloud services, such as Dynamics 365, cloud-based applications, and on-premises ERP and CRM applications. Its Intelligent Cloud segment provides Server products and cloud services, such as Azure and other cloud services, GitHub, Nuance Healthcare, virtual desktop offerings, and other cloud services; Server products, including SQL and Windows Server, Visual Studio and System Center related Client Access Licenses, and other on-premises offerings; Enterprise and partner services, including Enterprise Support and Nuance professional Services, Industry Solutions, Microsoft Partner Network, and Learning Experience. The company's Personal Computing segment provides Windows and Devices, such as Windows OEM licensing and Devices and Surface and PC accessories; Gaming services and solutions, such as Xbox hardware, content, and services, first- and third-party content Xbox Game Pass, subscriptions, and Cloud Gaming, advertising, and other cloud services; search and news advertising services, such as Bing and Copilot, Microsoft News and Edge, and third-party affiliates. It sells its products through OEMs, distributors, and resellers; and online and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.\", 'fullTimeEmployees': 228000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Satya  Nadella', 'age': 57, 'title': 'Chairman & CEO', 'yearBorn': 1967, 'fiscalYear': 2024, 'totalPay': 7869791, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Bradford L. Smith LCA', 'age': 65, 'title': 'President & Vice Chairman', 'yearBorn': 1959, 'fiscalYear': 2024, 'totalPay': 4755618, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Amy E. Hood', 'age': 52, 'title': 'Executive VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2024, 'totalPay': 4704250, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Carolina  Dybeck Happe', 'age': 52, 'title': 'Executive VP & COO', 'yearBorn': 1972, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Alice L. Jolla', 'age': 58, 'title': 'Corporate VP & Chief Accounting Officer', 'yearBorn': 1966, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Jonathan  Neilson', 'title': 'Vice President of Investor Relations', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Jonathan M. Palmer', 'title': 'Corporate Vice President & Chief Legal Officer', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Frank X. Shaw', 'title': 'Chief Communications Officer', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Takeshi  Numoto', 'age': 53, 'title': 'Executive VP & Chief Marketing Officer', 'yearBorn': 1971, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Keith Ranger Dolliver Esq.', 'title': 'VP, Deputy General Counsel of Corporate, External & Legal Affairs and Secretary', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 9, 'boardRisk': 5, 'compensationRisk': 4, 'shareHolderRightsRisk': 2, 'overallRisk': 3, 'governanceEpochDate': 1759276800, 'compensationAsOfEpochDate': 1735603200, 'irWebsite': 'http://www.microsoft.com/investor/default.aspx', 'executiveTeam': [], 'maxAge': 86400, 'priceHint': 2, 'previousClose': 519.71, 'open': 517.64, 'dayLow': 510.6791, 'dayHigh': 521.6, 'regularMarketPreviousClose': 519.71, 'regularMarketOpen': 517.64, 'regularMarketDayLow': 510.6791, 'regularMarketDayHigh': 521.6, 'dividendRate': 3.64, 'dividendYield': 0.7, 'exDividendDate': 1763596800, 'payoutRatio': 0.2375, 'fiveYearAvgDividendYield': 0.81, 'beta': 1.023, 'trailingPE': 37.47972, 'forwardPE': 34.270756, 'volume': 10082968, 'regularMarketVolume': 10082968, 'averageVolume': 20033825, 'averageVolume10days': 21667610, 'averageDailyVolume10Day': 21667610, 'bid': 509.03, 'ask': 516.49, 'bidSize': 4, 'askSize': 20, 'marketCap': 3808366166016, 'fiftyTwoWeekLow': 344.79, 'fiftyTwoWeekHigh': 555.45, 'allTimeHigh': 555.45, 'allTimeLow': 0.088542, 'priceToSalesTrailing12Months': 13.518075, 'fiftyDayAverage': 512.6394, 'twoHundredDayAverage': 451.29376, 'trailingAnnualDividendRate': 3.32, 'trailingAnnualDividendYield': 0.0063881776, 'currency': 'USD', 'tradeable': False, 'enterpriseValue': 3880709783552, 'profitMargins': 0.36146, 'floatShares': 7422239624, 'sharesOutstanding': 7433166379, 'sharesShort': 65238937, 'sharesShortPriorMonth': 56182641, 'sharesShortPreviousMonthDate': 1755216000, 'dateShortInterest': 1757894400, 'sharesPercentSharesOut': 0.0088, 'heldPercentInsiders': 0.00068000006, 'heldPercentInstitutions': 0.74547994, 'shortRatio': 3.09, 'shortPercentOfFloat': 0.0088, 'impliedSharesOutstanding': 7433166379, 'bookValue': 46.204, 'priceToBook': 11.088819, 'lastFiscalYearEnd': 1751241600, 'nextFiscalYearEnd': 1782777600, 'mostRecentQuarter': 1751241600, 'earningsQuarterlyGrowth': 0.236, 'netIncomeToCommon': 101831999488, 'trailingEps': 13.67, 'forwardEps': 14.95, 'lastSplitFactor': '2:1', 'lastSplitDate': 1045526400, 'enterpriseToRevenue': 13.775, 'enterpriseToEbitda': 24.792, '52WeekChange': 0.24768329, 'SandP52WeekChange': 0.17741585, 'lastDividendValue': 0.83, 'lastDividendDate': 1755734400, 'quoteType': 'EQUITY', 'currentPrice': 512.3478, 'targetHighPrice': 710.0, 'targetLowPrice': 483.0, 'targetMeanPrice': 617.5016, 'targetMedianPrice': 630.0, 'recommendationMean': 1.25862, 'recommendationKey': 'strong_buy', 'numberOfAnalystOpinions': 51, 'totalCash': 94564999168, 'totalCashPerShare': 12.722, 'ebitda': 156528001024, 'totalDebt': 112184000512, 'quickRatio': 1.223, 'currentRatio': 1.353, 'totalRevenue': 281723994112, 'debtToEquity': 32.661, 'revenuePerShare': 37.902, 'returnOnAssets': 0.14203, 'returnOnEquity': 0.33280998, 'grossProfits': 193893007360, 'freeCashflow': 61070376960, 'operatingCashflow': 136162000896, 'earningsGrowth': 0.237, 'revenueGrowth': 0.181, 'grossMargins': 0.68824, 'ebitdaMargins': 0.55561, 'operatingMargins': 0.44901, 'financialCurrency': 'USD', 'symbol': 'MSFT', 'language': 'en-US', 'region': 'US', 'typeDisp': 'Equity', 'quoteSourceName': 'Nasdaq Real Time Price', 'triggerable': True, 'customPriceAlertConfidence': 'HIGH', 'corporateActions': [], 'regularMarketTime': 1759419841, 'exchange': 'NMS', 'messageBoardId': 'finmb_21835', 'exchangeTimezoneName': 'America/New_York', 'exchangeTimezoneShortName': 'EDT', 'gmtOffSetMilliseconds': -14400000, 'market': 'us_market', 'esgPopulated': False, 'averageAnalystRating': '1.3 - Strong Buy', 'cryptoTradeable': False, 'regularMarketChange': -7.3622437, 'regularMarketDayRange': '510.6791 - 521.6', 'fullExchangeName': 'NasdaqGS', 'averageDailyVolume3Month': 20033825, 'fiftyTwoWeekLowChange': 167.55777, 'fiftyTwoWeekLowChangePercent': 0.4859705, 'fiftyTwoWeekRange': '344.79 - 555.45', 'fiftyTwoWeekHighChange': -43.102234, 'fiftyTwoWeekHighChangePercent': -0.077598765, 'fiftyTwoWeekChangePercent': 24.76833, 'dividendDate': 1765411200, 'earningsTimestamp': 1753905600, 'earningsTimestampStart': 1761768000, 'earningsTimestampEnd': 1761768000, 'earningsCallTimestampStart': 1753911000, 'earningsCallTimestampEnd': 1753911000, 'isEarningsDateEstimate': False, 'epsTrailingTwelveMonths': 13.67, 'epsForward': 14.95, 'epsCurrentYear': 15.55376, 'priceEpsCurrentYear': 32.940445, 'fiftyDayAverageChange': -0.29162598, 'fiftyDayAverageChangePercent': -0.00056887156, 'twoHundredDayAverageChange': 61.054016, 'twoHundredDayAverageChangePercent': 0.13528664, 'sourceInterval': 15, 'exchangeDataDelayedBy': 0, 'hasPrePostMarketData': True, 'firstTradeDateMilliseconds': 511108200000, 'marketState': 'REGULAR', 'shortName': 'Microsoft Corporation', 'longName': 'Microsoft Corporation', 'regularMarketChangePercent': -1.4166061, 'regularMarketPrice': 512.3478, 'displayName': 'Microsoft', 'trailingPegRatio': 2.269}\n",
      "{'current': 512.3478, 'high': 710.0, 'low': 483.0, 'mean': 617.5016, 'median': 630.0}\n",
      "  period  strongBuy  buy  hold  sell  strongSell\n",
      "0     0m         13   44     1     0           0\n",
      "1    -1m         13   44     1     0           0\n",
      "2    -2m         12   43     1     0           0\n",
      "3    -3m         12   43     3     0           0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T16:40:23.519537Z",
     "start_time": "2025-10-02T16:40:19.246135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enhanced stock data downloader with price + fundamentals from yfinance.\n",
    "Outputs tidy long tables for easy aggregation with improved reliability.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Union\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "# Suppress yfinance warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DownloadConfig:\n",
    "    \"\"\"Configuration for data download parameters.\"\"\"\n",
    "    start: str = \"2000-01-01\"\n",
    "    end: Optional[str] = None\n",
    "    interval: str = \"1d\"\n",
    "    out_dir: str = \"./out\"\n",
    "    max_workers: int = 4\n",
    "    request_delay: float = 0.3\n",
    "    auto_adjust: bool = False\n",
    "    timeout: int = 30\n",
    "    max_retries: int = 2\n",
    "\n",
    "class StockDataDownloader:\n",
    "    \"\"\"Main class for downloading and processing stock data.\"\"\"\n",
    "\n",
    "    def __init__(self, config: DownloadConfig):\n",
    "        self.config = config\n",
    "        os.makedirs(config.out_dir, exist_ok=True)\n",
    "\n",
    "    def to_long_prices(self, df_multi: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert yf.download wide MultiIndex columns to tidy long format.\"\"\"\n",
    "        if df_multi.empty:\n",
    "            return pd.DataFrame(columns=[\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"])\n",
    "\n",
    "        # Handle single ticker case (no MultiIndex)\n",
    "        if not isinstance(df_multi.columns, pd.MultiIndex):\n",
    "            df_long = df_multi.reset_index()\n",
    "            df_long[\"ticker\"] = \"SINGLE\"  # Placeholder, will be updated later\n",
    "            # Standardize column names\n",
    "            df_long = df_long.rename(columns={\n",
    "                \"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\",\n",
    "                \"Close\": \"close\", \"Adj Close\": \"adj_close\", \"Volume\": \"volume\"\n",
    "            })\n",
    "        else:\n",
    "            # Multiple tickers case\n",
    "            df_long = df_multi.stack(level=0).reset_index()\n",
    "            df_long = df_long.rename(columns={\n",
    "                \"level_0\": \"date\", \"level_1\": \"ticker\"\n",
    "            })\n",
    "            # Standardize column names\n",
    "            rename_map = {\n",
    "                \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\",\n",
    "                \"Adj Close\": \"adj_close\", \"Volume\": \"volume\"\n",
    "            }\n",
    "            df_long = df_long.rename(columns=rename_map)\n",
    "\n",
    "        # Ensure consistent column order and types\n",
    "        expected_cols = [\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "        for col in expected_cols:\n",
    "            if col not in df_long.columns:\n",
    "                df_long[col] = None\n",
    "\n",
    "        df_long = df_long[expected_cols]\n",
    "        df_long[\"date\"] = pd.to_datetime(df_long[\"date\"])\n",
    "\n",
    "        return df_long\n",
    "\n",
    "    def safe_get(self, data, key, default=None, data_type=None):\n",
    "        \"\"\"Safely get value from dictionary or object with type conversion.\"\"\"\n",
    "        try:\n",
    "            if isinstance(data, dict):\n",
    "                value = data.get(key, default)\n",
    "            else:\n",
    "                value = getattr(data, key, default)\n",
    "\n",
    "            # Type conversion\n",
    "            if data_type and value is not None:\n",
    "                try:\n",
    "                    if data_type == \"float\":\n",
    "                        value = float(value)\n",
    "                    elif data_type == \"int\":\n",
    "                        value = int(value)\n",
    "                    elif data_type == \"str\":\n",
    "                        value = str(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    value = default\n",
    "\n",
    "            return value\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def flatten_info(self, ticker_obj: yf.Ticker) -> dict:\n",
    "        \"\"\"Collect comprehensive snapshot from info and fast_info.\"\"\"\n",
    "        out = {\"ticker\": ticker_obj.ticker}\n",
    "\n",
    "        # Get info with error handling\n",
    "        try:\n",
    "            info = ticker_obj.info or {}\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to get info for {ticker_obj.ticker}: {e}\")\n",
    "            info = {}\n",
    "\n",
    "        # Get fast_info with error handling\n",
    "        try:\n",
    "            fast_info = ticker_obj.fast_info or {}\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Failed to get fast_info for {ticker_obj.ticker}: {e}\")\n",
    "            fast_info = {}\n",
    "\n",
    "        # Define fields to extract with their types\n",
    "        info_fields = {\n",
    "            # Basic identity\n",
    "            \"shortName\": (\"str\", None),\n",
    "            \"longName\": (\"str\", None),\n",
    "            \"country\": (\"str\", None),\n",
    "            \"sector\": (\"str\", None),\n",
    "            \"industry\": (\"str\", None),\n",
    "            \"currency\": (\"str\", None),\n",
    "            \"exchange\": (\"str\", None),\n",
    "            \"quoteType\": (\"str\", None),\n",
    "\n",
    "            # Valuation\n",
    "            \"marketCap\": (\"float\", None),\n",
    "            \"trailingPE\": (\"float\", None),\n",
    "            \"forwardPE\": (\"float\", None),\n",
    "            \"priceToBook\": (\"float\", None),\n",
    "            \"enterpriseValue\": (\"float\", None),\n",
    "            \"enterpriseToRevenue\": (\"float\", None),\n",
    "            \"enterpriseToEbitda\": (\"float\", None),\n",
    "            \"beta\": (\"float\", None),\n",
    "            \"pegRatio\": (\"float\", None),\n",
    "\n",
    "            # Margins\n",
    "            \"profitMargins\": (\"float\", None),\n",
    "            \"grossMargins\": (\"float\", None),\n",
    "            \"operatingMargins\": (\"float\", None),\n",
    "            \"ebitdaMargins\": (\"float\", None),\n",
    "\n",
    "            # Per-share metrics\n",
    "            \"trailingEps\": (\"float\", None),\n",
    "            \"forwardEps\": (\"float\", None),\n",
    "            \"bookValue\": (\"float\", None),\n",
    "            \"revenuePerShare\": (\"float\", None),\n",
    "\n",
    "            # Growth\n",
    "            \"earningsQuarterlyGrowth\": (\"float\", None),\n",
    "            \"revenueGrowth\": (\"float\", None),\n",
    "\n",
    "            # Financials\n",
    "            \"totalCash\": (\"float\", None),\n",
    "            \"totalDebt\": (\"float\", None),\n",
    "            \"totalRevenue\": (\"float\", None),\n",
    "            \"ebitda\": (\"float\", None),\n",
    "            \"freeCashflow\": (\"float\", None),\n",
    "            \"operatingCashflow\": (\"float\", None),\n",
    "\n",
    "            # Ratios\n",
    "            \"debtToEquity\": (\"float\", None),\n",
    "            \"currentRatio\": (\"float\", None),\n",
    "            \"quickRatio\": (\"float\", None),\n",
    "\n",
    "            # Dividends\n",
    "            \"dividendRate\": (\"float\", None),\n",
    "            \"dividendYield\": (\"float\", None),\n",
    "            \"payoutRatio\": (\"float\", None),\n",
    "        }\n",
    "\n",
    "        # Extract info fields\n",
    "        for field, (data_type, default) in info_fields.items():\n",
    "            out[field] = self.safe_get(info, field, default, data_type)\n",
    "\n",
    "        # Extract fast_info fields\n",
    "        fast_info_fields = [\n",
    "            \"lastPrice\", \"dayHigh\", \"dayLow\", \"yearHigh\", \"yearLow\",\n",
    "            \"previousClose\", \"open\", \"marketCap\", \"shares\",\n",
    "            \"volume\", \"averageVolume10Day\", \"averageVolume3Month\"\n",
    "        ]\n",
    "\n",
    "        for field in fast_info_fields:\n",
    "            out[f\"fast_{field}\"] = self.safe_get(fast_info, field, None, \"float\")\n",
    "\n",
    "        # Name fallback - ensure longName always exists\n",
    "        if not out.get(\"longName\"):\n",
    "            out[\"longName\"] = out.get(\"shortName\") or out[\"ticker\"]\n",
    "\n",
    "        # Add timestamp\n",
    "        out[\"snapshot_timestamp\"] = pd.Timestamp.now()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def melt_statements(self, df: pd.DataFrame, ticker: str, period: str, statement: str) -> pd.DataFrame:\n",
    "        \"\"\"Convert wide financial statements to long format.\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame(columns=[\"ticker\", \"as_of_date\", \"period\", \"statement\", \"item\", \"value\"])\n",
    "\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            # Convert column names to datetime\n",
    "            df.columns = pd.to_datetime(df.columns, errors=\"coerce\")\n",
    "\n",
    "            # Reset index and melt\n",
    "            df = df.reset_index().rename(columns={\"index\": \"item\"})\n",
    "            df = df.melt(\n",
    "                id_vars=[\"item\"],\n",
    "                var_name=\"as_of_date\",\n",
    "                value_name=\"value\"\n",
    "            )\n",
    "\n",
    "            # Add metadata\n",
    "            df[\"ticker\"] = ticker\n",
    "            df[\"period\"] = period\n",
    "            df[\"statement\"] = statement\n",
    "\n",
    "            # Filter and sort\n",
    "            df = df.dropna(subset=[\"as_of_date\", \"item\"])\n",
    "            df = df[[\"ticker\", \"as_of_date\", \"period\", \"statement\", \"item\", \"value\"]]\n",
    "            df = df.sort_values([\"ticker\", \"as_of_date\", \"statement\", \"item\"])\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error melting statements for {ticker}: {e}\")\n",
    "            return pd.DataFrame(columns=[\"ticker\", \"as_of_date\", \"period\", \"statement\", \"item\", \"value\"])\n",
    "\n",
    "    def safe_download_data(self, ticker_obj, data_type, retries=2):\n",
    "        \"\"\"Safely download data with retry logic.\"\"\"\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                if data_type == \"financials\":\n",
    "                    result = ticker_obj.financials\n",
    "                elif data_type == \"balance_sheet\":\n",
    "                    result = ticker_obj.balance_sheet\n",
    "                elif data_type == \"cashflow\":\n",
    "                    result = ticker_obj.cashflow\n",
    "                elif data_type == \"quarterly_financials\":\n",
    "                    result = ticker_obj.quarterly_financials\n",
    "                elif data_type == \"quarterly_balance_sheet\":\n",
    "                    result = ticker_obj.quarterly_balance_sheet\n",
    "                elif data_type == \"quarterly_cashflow\":\n",
    "                    result = ticker_obj.quarterly_cashflow\n",
    "                elif data_type == \"dividends\":\n",
    "                    result = ticker_obj.dividends\n",
    "                elif data_type == \"splits\":\n",
    "                    result = ticker_obj.splits\n",
    "                elif data_type == \"analyst_price_targets\":\n",
    "                    result = ticker_obj.analyst_price_targets\n",
    "                elif data_type == \"recommendations\":\n",
    "                    result = ticker_obj.recommendations\n",
    "                elif data_type == \"institutional_holders\":\n",
    "                    result = ticker_obj.institutional_holders\n",
    "                elif data_type == \"funds_holding\":\n",
    "                    result = ticker_obj.funds_holding\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "                # Handle different return types from yfinance\n",
    "                if result is None:\n",
    "                    return pd.DataFrame()\n",
    "                elif isinstance(result, dict):\n",
    "                    logger.warning(f\"yfinance returned dict for {data_type} on {ticker_obj.ticker}, converting to DataFrame\")\n",
    "                    return pd.DataFrame()\n",
    "                elif hasattr(result, 'empty'):\n",
    "                    return result\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected type {type(result)} for {data_type} on {ticker_obj.ticker}\")\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    logger.debug(f\"Failed to get {data_type} for {ticker_obj.ticker}: {e}\")\n",
    "                    return pd.DataFrame()\n",
    "                time.sleep(self.config.request_delay * 2)  # Longer delay between retries\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def download_ticker_fundamentals(self, ticker: str) -> dict:\n",
    "        \"\"\"Download fundamentals for a single ticker.\"\"\"\n",
    "        logger.info(f\"Downloading fundamentals for {ticker}\")\n",
    "\n",
    "        time.sleep(self.config.request_delay)\n",
    "\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "\n",
    "            # Get snapshot data\n",
    "            snapshot = self.flatten_info(ticker_obj)\n",
    "\n",
    "            # Download financial statements\n",
    "            statements_data = {}\n",
    "            statement_types = [\n",
    "                (\"financials\", \"annual\", \"income\"),\n",
    "                (\"balance_sheet\", \"annual\", \"balance\"),\n",
    "                (\"cashflow\", \"annual\", \"cashflow\"),\n",
    "                (\"quarterly_financials\", \"quarterly\", \"income\"),\n",
    "                (\"quarterly_balance_sheet\", \"quarterly\", \"balance\"),\n",
    "                (\"quarterly_cashflow\", \"quarterly\", \"cashflow\"),\n",
    "            ]\n",
    "\n",
    "            statements_long = []\n",
    "            for data_type, period, statement in statement_types:\n",
    "                df = self.safe_download_data(ticker_obj, data_type)\n",
    "                # Check if we have a valid DataFrame before melting\n",
    "                if df is not None and hasattr(df, 'empty') and not df.empty:\n",
    "                    melted_df = self.melt_statements(df, ticker, period, statement)\n",
    "                    if not melted_df.empty:\n",
    "                        statements_long.append(melted_df)\n",
    "\n",
    "            # Download dividends and splits\n",
    "            dividends_df = self.safe_download_data(ticker_obj, \"dividends\")\n",
    "            splits_df = self.safe_download_data(ticker_obj, \"splits\")\n",
    "\n",
    "            # Process dividends - handle Series and DataFrame returns\n",
    "            dividends = pd.DataFrame(columns=[\"as_of_date\", \"dividend\", \"ticker\"])\n",
    "            if dividends_df is not None:\n",
    "                if hasattr(dividends_df, 'empty') and not dividends_df.empty:\n",
    "                    if hasattr(dividends_df, 'reset_index'):\n",
    "                        dividends = dividends_df.reset_index().rename(\n",
    "                            columns={\"Date\": \"as_of_date\", \"Dividends\": \"dividend\"}\n",
    "                        )\n",
    "                        dividends[\"ticker\"] = ticker\n",
    "                    elif isinstance(dividends_df, pd.Series):\n",
    "                        dividends = dividends_df.reset_index().rename(\n",
    "                            columns={\"index\": \"as_of_date\", 0: \"dividend\"}\n",
    "                        )\n",
    "                        dividends[\"ticker\"] = ticker\n",
    "\n",
    "            # Process splits\n",
    "            splits = pd.DataFrame(columns=[\"as_of_date\", \"split_ratio\", \"ticker\"])\n",
    "            if splits_df is not None:\n",
    "                if hasattr(splits_df, 'empty') and not splits_df.empty:\n",
    "                    if hasattr(splits_df, 'reset_index'):\n",
    "                        splits = splits_df.reset_index().rename(\n",
    "                            columns={\"Date\": \"as_of_date\", \"Stock Splits\": \"split_ratio\"}\n",
    "                        )\n",
    "                        splits[\"ticker\"] = ticker\n",
    "                    elif isinstance(splits_df, pd.Series):\n",
    "                        splits = splits_df.reset_index().rename(\n",
    "                            columns={\"index\": \"as_of_date\", 0: \"split_ratio\"}\n",
    "                        )\n",
    "                        splits[\"ticker\"] = ticker\n",
    "\n",
    "            # Download additional data\n",
    "            analyst_data = self.safe_download_data(ticker_obj, \"analyst_price_targets\")\n",
    "            recommendations = self.safe_download_data(ticker_obj, \"recommendations\")\n",
    "            institutional_holders = self.safe_download_data(ticker_obj, \"institutional_holders\")\n",
    "            funds_holding = self.safe_download_data(ticker_obj, \"funds_holding\")\n",
    "\n",
    "            # Add ticker to analyst data if missing\n",
    "            for df, name in [\n",
    "                (analyst_data, \"analyst_price_targets\"),\n",
    "                (recommendations, \"recommendations\"),\n",
    "                (institutional_holders, \"institutional_holders\"),\n",
    "                (funds_holding, \"funds_holding\")\n",
    "            ]:\n",
    "                if df is not None and hasattr(df, 'empty') and not df.empty and \"symbol\" not in df.columns:\n",
    "                    df = df.copy()\n",
    "                    df[\"symbol\"] = ticker\n",
    "\n",
    "            return {\n",
    "                \"snapshot\": snapshot,\n",
    "                \"statements_long\": pd.concat(statements_long, ignore_index=True) if statements_long else pd.DataFrame(),\n",
    "                \"dividends\": dividends,\n",
    "                \"splits\": splits,\n",
    "                \"analyst_price_targets\": analyst_data,\n",
    "                \"recommendations\": recommendations,\n",
    "                \"institutional_holders\": institutional_holders,\n",
    "                \"funds_holding\": funds_holding,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading fundamentals for {ticker}: {e}\")\n",
    "            return self._create_empty_fundamentals_result(ticker)\n",
    "\n",
    "    def _create_empty_fundamentals_result(self, ticker: str) -> dict:\n",
    "        \"\"\"Create empty result structure for failed downloads.\"\"\"\n",
    "        # Ensure longName is always present in snapshot\n",
    "        snapshot = {\"ticker\": ticker, \"longName\": ticker}\n",
    "        return {\n",
    "            \"snapshot\": snapshot,\n",
    "            \"statements_long\": pd.DataFrame(columns=[\"ticker\", \"as_of_date\", \"period\", \"statement\", \"item\", \"value\"]),\n",
    "            \"dividends\": pd.DataFrame(columns=[\"as_of_date\", \"dividend\", \"ticker\"]),\n",
    "            \"splits\": pd.DataFrame(columns=[\"as_of_date\", \"split_ratio\", \"ticker\"]),\n",
    "            \"analyst_price_targets\": pd.DataFrame(),\n",
    "            \"recommendations\": pd.DataFrame(),\n",
    "            \"institutional_holders\": pd.DataFrame(),\n",
    "            \"funds_holding\": pd.DataFrame(),\n",
    "        }\n",
    "\n",
    "    def safe_create_name_map(self, snapshot_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        \"\"\"Safely create a mapping from ticker to company name.\"\"\"\n",
    "        name_map = {}\n",
    "\n",
    "        if snapshot_df.empty:\n",
    "            return name_map\n",
    "\n",
    "        # Ensure ticker column exists\n",
    "        if \"ticker\" not in snapshot_df.columns:\n",
    "            logger.warning(\"ticker column missing from snapshot_df\")\n",
    "            return name_map\n",
    "\n",
    "        # Try different possible name columns\n",
    "        name_columns = ['longName', 'shortName', 'ticker']\n",
    "\n",
    "        for name_col in name_columns:\n",
    "            if name_col in snapshot_df.columns:\n",
    "                try:\n",
    "                    for _, row in snapshot_df.iterrows():\n",
    "                        ticker = row[\"ticker\"]\n",
    "                        name = row.get(name_col, ticker)\n",
    "                        if ticker not in name_map or name_map[ticker] == ticker:\n",
    "                            name_map[ticker] = name if name and pd.notna(name) else ticker\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error creating name map from {name_col}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Ensure all tickers in snapshot_df have an entry\n",
    "        for ticker in snapshot_df[\"ticker\"].unique():\n",
    "            if ticker not in name_map:\n",
    "                name_map[ticker] = ticker\n",
    "\n",
    "        return name_map\n",
    "\n",
    "    def save_dataframe(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"Save DataFrame in CSV format (fallback from Parquet).\"\"\"\n",
    "        if df.empty:\n",
    "            logger.info(f\"Skipping empty dataframe: {filename}\")\n",
    "            return\n",
    "\n",
    "        base_path = os.path.join(self.config.out_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Try to save as CSV (more compatible than Parquet)\n",
    "            csv_path = f\"{base_path}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Saved {len(df)} rows to {csv_path}\")\n",
    "\n",
    "            # Try to save as Parquet if dependencies are available\n",
    "            try:\n",
    "                parquet_path = f\"{base_path}.parquet\"\n",
    "                df.to_parquet(parquet_path, index=False)\n",
    "                logger.info(f\"Saved {len(df)} rows to {parquet_path}\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Parquet save skipped for {filename}: {e}. Using CSV only.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving {filename}: {e}\")\n",
    "\n",
    "    def run(self, tickers: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Main method to download and process all data.\"\"\"\n",
    "        logger.info(f\"Starting data download for {len(tickers)} tickers\")\n",
    "\n",
    "        # 1) Download price data\n",
    "        logger.info(\"Downloading price data...\")\n",
    "        try:\n",
    "            price_data = yf.download(\n",
    "                tickers,\n",
    "                start=self.config.start,\n",
    "                end=self.config.end,\n",
    "                interval=self.config.interval,\n",
    "                group_by=\"ticker\",\n",
    "                auto_adjust=self.config.auto_adjust,\n",
    "                threads=True,\n",
    "                progress=False,\n",
    "                timeout=self.config.timeout\n",
    "            )\n",
    "\n",
    "            prices_long = self.to_long_prices(price_data)\n",
    "\n",
    "            # Handle single ticker case\n",
    "            if len(tickers) == 1 and \"ticker\" in prices_long.columns:\n",
    "                prices_long[\"ticker\"] = tickers[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading price data: {e}\")\n",
    "            prices_long = pd.DataFrame(columns=[\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"])\n",
    "\n",
    "        # 2) Download fundamentals in parallel\n",
    "        logger.info(\"Downloading fundamentals data...\")\n",
    "        snapshots = []\n",
    "        statements_buf = []\n",
    "        dividends_buf = []\n",
    "        splits_buf = []\n",
    "        pt_buf, reco_buf, inst_buf, funds_buf = [], [], [], []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
    "            future_to_ticker = {\n",
    "                executor.submit(self.download_ticker_fundamentals, ticker): ticker\n",
    "                for ticker in tickers\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_ticker):\n",
    "                ticker = future_to_ticker[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to process {ticker}: {e}\")\n",
    "                    result = self._create_empty_fundamentals_result(ticker)\n",
    "\n",
    "                snapshots.append(result[\"snapshot\"])\n",
    "\n",
    "                if not result[\"statements_long\"].empty:\n",
    "                    statements_buf.append(result[\"statements_long\"])\n",
    "                if not result[\"dividends\"].empty:\n",
    "                    dividends_buf.append(result[\"dividends\"])\n",
    "                if not result[\"splits\"].empty:\n",
    "                    splits_buf.append(result[\"splits\"])\n",
    "                if not result[\"analyst_price_targets\"].empty:\n",
    "                    pt_buf.append(result[\"analyst_price_targets\"])\n",
    "                if not result[\"recommendations\"].empty:\n",
    "                    reco_buf.append(result[\"recommendations\"])\n",
    "                if not result[\"institutional_holders\"].empty:\n",
    "                    inst_buf.append(result[\"institutional_holders\"])\n",
    "                if not result[\"funds_holding\"].empty:\n",
    "                    funds_buf.append(result[\"funds_holding\"])\n",
    "\n",
    "        # 3) Combine all data\n",
    "        logger.info(\"Combining and saving data...\")\n",
    "\n",
    "        # Create snapshot DataFrame\n",
    "        snapshot_df = pd.DataFrame(snapshots)\n",
    "        if not snapshot_df.empty:\n",
    "            snapshot_df = snapshot_df.drop_duplicates(subset=[\"ticker\"])\n",
    "            # Safely create name mapping\n",
    "            name_map = self.safe_create_name_map(snapshot_df)\n",
    "            prices_long[\"company\"] = prices_long[\"ticker\"].map(name_map)\n",
    "        else:\n",
    "            prices_long[\"company\"] = prices_long[\"ticker\"]\n",
    "\n",
    "        # Combine statement data\n",
    "        statements_long = pd.concat(statements_buf, ignore_index=True) if statements_buf else pd.DataFrame()\n",
    "        dividends_long = pd.concat(dividends_buf, ignore_index=True) if dividends_buf else pd.DataFrame()\n",
    "        splits_long = pd.concat(splits_buf, ignore_index=True) if splits_buf else pd.DataFrame()\n",
    "\n",
    "        # Combine analyst data\n",
    "        price_targets = pd.concat(pt_buf, ignore_index=True) if pt_buf else pd.DataFrame()\n",
    "        recommendations = pd.concat(reco_buf, ignore_index=True) if reco_buf else pd.DataFrame()\n",
    "        institutional_holders = pd.concat(inst_buf, ignore_index=True) if inst_buf else pd.DataFrame()\n",
    "        funds_holding = pd.concat(funds_buf, ignore_index=True) if funds_buf else pd.DataFrame()\n",
    "\n",
    "        # 4) Save all data\n",
    "        self.save_dataframe(prices_long, \"prices_long\")\n",
    "        self.save_dataframe(snapshot_df, \"fundamentals_snapshot\")\n",
    "\n",
    "        if not statements_long.empty:\n",
    "            self.save_dataframe(statements_long, \"statements_long\")\n",
    "        if not dividends_long.empty:\n",
    "            self.save_dataframe(dividends_long, \"dividends_long\")\n",
    "        if not splits_long.empty:\n",
    "            self.save_dataframe(splits_long, \"splits_long\")\n",
    "        if not price_targets.empty:\n",
    "            self.save_dataframe(price_targets, \"analyst_price_targets\")\n",
    "        if not recommendations.empty:\n",
    "            self.save_dataframe(recommendations, \"recommendations\")\n",
    "        if not institutional_holders.empty:\n",
    "            self.save_dataframe(institutional_holders, \"institutional_holders\")\n",
    "        if not funds_holding.empty:\n",
    "            self.save_dataframe(funds_holding, \"funds_holding\")\n",
    "\n",
    "        logger.info(f\"Data download completed. Results saved to: {os.path.abspath(self.config.out_dir)}\")\n",
    "\n",
    "        return {\n",
    "            \"prices_long\": prices_long,\n",
    "            \"fundamentals_snapshot\": snapshot_df,\n",
    "            \"statements_long\": statements_long,\n",
    "            \"dividends_long\": dividends_long,\n",
    "            \"splits_long\": splits_long,\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage.\"\"\"\n",
    "    config = DownloadConfig(\n",
    "        start=\"2023-01-01\",\n",
    "        end=None,\n",
    "        interval=\"1d\",\n",
    "        out_dir=\"./stock_data\",\n",
    "        max_workers=4,\n",
    "        request_delay=0.4,\n",
    "        auto_adjust=False\n",
    "    )\n",
    "\n",
    "    downloader = StockDataDownloader(config)\n",
    "\n",
    "    # Example tickers\n",
    "    tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"TSLA\"]\n",
    "\n",
    "    results = downloader.run(tickers)\n",
    "\n",
    "    # Print summary\n",
    "    for key, df in results.items():\n",
    "        print(f\"{key}: {len(df)} rows\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "cde53b00ff0089f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 11:40:19,281 - INFO - Starting data download for 4 tickers\n",
      "2025-10-02 11:40:19,282 - INFO - Downloading price data...\n",
      "2025-10-02 11:40:19,504 - INFO - Downloading fundamentals data...\n",
      "2025-10-02 11:40:19,505 - INFO - Downloading fundamentals for AAPL\n",
      "2025-10-02 11:40:19,505 - INFO - Downloading fundamentals for MSFT\n",
      "2025-10-02 11:40:19,506 - INFO - Downloading fundamentals for GOOGL\n",
      "2025-10-02 11:40:19,506 - INFO - Downloading fundamentals for TSLA\n",
      "2025-10-02 11:40:20,206 - ERROR - HTTP Error 401: {\"finance\":{\"result\":null,\"error\":{\"code\":\"Unauthorized\",\"description\":\"Invalid Crumb\"}}}\n",
      "2025-10-02 11:40:20,239 - ERROR - HTTP Error 401: {\"finance\":{\"result\":null,\"error\":{\"code\":\"Unauthorized\",\"description\":\"Invalid Crumb\"}}}\n",
      "2025-10-02 11:40:21,047 - WARNING - yfinance returned dict for analyst_price_targets on GOOGL, converting to DataFrame\n",
      "2025-10-02 11:40:21,333 - WARNING - yfinance returned dict for analyst_price_targets on AAPL, converting to DataFrame\n",
      "2025-10-02 11:40:21,334 - WARNING - yfinance returned dict for analyst_price_targets on TSLA, converting to DataFrame\n",
      "2025-10-02 11:40:21,695 - WARNING - yfinance returned dict for analyst_price_targets on MSFT, converting to DataFrame\n",
      "2025-10-02 11:40:23,444 - INFO - Combining and saving data...\n",
      "2025-10-02 11:40:23,481 - INFO - Saved 2759 rows to ./stock_data/prices_long.csv\n",
      "2025-10-02 11:40:23,482 - INFO - Parquet save skipped for prices_long: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,484 - INFO - Saved 4 rows to ./stock_data/fundamentals_snapshot.csv\n",
      "2025-10-02 11:40:23,484 - INFO - Parquet save skipped for fundamentals_snapshot: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,508 - INFO - Saved 7143 rows to ./stock_data/statements_long.csv\n",
      "2025-10-02 11:40:23,509 - INFO - Parquet save skipped for statements_long: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,511 - INFO - Saved 181 rows to ./stock_data/dividends_long.csv\n",
      "2025-10-02 11:40:23,512 - INFO - Parquet save skipped for dividends_long: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,513 - INFO - Saved 18 rows to ./stock_data/splits_long.csv\n",
      "2025-10-02 11:40:23,513 - INFO - Parquet save skipped for splits_long: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,514 - INFO - Saved 16 rows to ./stock_data/recommendations.csv\n",
      "2025-10-02 11:40:23,515 - INFO - Parquet save skipped for recommendations: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,516 - INFO - Saved 40 rows to ./stock_data/institutional_holders.csv\n",
      "2025-10-02 11:40:23,517 - INFO - Parquet save skipped for institutional_holders: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.. Using CSV only.\n",
      "2025-10-02 11:40:23,517 - INFO - Data download completed. Results saved to: /Users/horizon/PycharmProjects/Horisation/Backend/Sandbox/Stock/stock_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices_long: 2759 rows\n",
      "fundamentals_snapshot: 4 rows\n",
      "statements_long: 7143 rows\n",
      "dividends_long: 181 rows\n",
      "splits_long: 18 rows\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T23:20:05.645598Z",
     "start_time": "2025-10-02T23:20:05.592988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "price_df = pd.read_csv('price_history.csv')\n",
    "price_df"
   ],
   "id": "bc2921f791f829b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             Date           Sub Theme              Company Ticker        Open  \\\n",
       "0      2021-01-13  Credit Card & BNPL  AFFIRM HOLDINGS INC   AFRM   90.900002   \n",
       "1      2021-01-14  Credit Card & BNPL  AFFIRM HOLDINGS INC   AFRM  103.500000   \n",
       "2      2021-01-15  Credit Card & BNPL  AFFIRM HOLDINGS INC   AFRM  123.054001   \n",
       "3      2021-01-19  Credit Card & BNPL  AFFIRM HOLDINGS INC   AFRM  117.000000   \n",
       "4      2021-01-20  Credit Card & BNPL  AFFIRM HOLDINGS INC   AFRM  110.000000   \n",
       "...           ...                 ...                  ...    ...         ...   \n",
       "25993  2025-09-26    Fintech Hardware   SQUARE (BLOCK INC)    XYZ   73.385002   \n",
       "25994  2025-09-29      Cryptocurrency            BLOCK INC    XYZ   74.449997   \n",
       "25995  2025-09-29    Fintech Hardware   SQUARE (BLOCK INC)    XYZ   74.449997   \n",
       "25996  2025-09-30      Cryptocurrency            BLOCK INC    XYZ   75.320000   \n",
       "25997  2025-09-30    Fintech Hardware   SQUARE (BLOCK INC)    XYZ   75.320000   \n",
       "\n",
       "             High         Low       Close   Adj Close      Volume       r_t  \n",
       "0      103.000000   90.010002   96.364998   96.364998  25159000.0       NaN  \n",
       "1      137.979996  101.000000  114.940002  114.940002  20858700.0  0.176267  \n",
       "2      127.239998  107.000000  117.000000  117.000000   8310700.0  0.017764  \n",
       "3      117.675003  108.000000  110.989998  110.989998   3304900.0 -0.052734  \n",
       "4      112.000000  103.250000  106.209999  106.209999   5091400.0 -0.044022  \n",
       "...           ...         ...         ...         ...         ...       ...  \n",
       "25993   73.830002   71.440002   73.669998   73.669998   6578200.0  0.000000  \n",
       "25994   76.154999   74.334999   75.389999   75.389999   7237200.0  0.023079  \n",
       "25995   76.154999   74.334999   75.389999   75.389999   7237200.0  0.000000  \n",
       "25996   75.665001   71.610001   72.269997   72.269997   7838100.0 -0.042266  \n",
       "25997   75.665001   71.610001   72.269997   72.269997   7838100.0  0.000000  \n",
       "\n",
       "[25998 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sub Theme</th>\n",
       "      <th>Company</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>r_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>Credit Card &amp; BNPL</td>\n",
       "      <td>AFFIRM HOLDINGS INC</td>\n",
       "      <td>AFRM</td>\n",
       "      <td>90.900002</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>90.010002</td>\n",
       "      <td>96.364998</td>\n",
       "      <td>96.364998</td>\n",
       "      <td>25159000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>Credit Card &amp; BNPL</td>\n",
       "      <td>AFFIRM HOLDINGS INC</td>\n",
       "      <td>AFRM</td>\n",
       "      <td>103.500000</td>\n",
       "      <td>137.979996</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>114.940002</td>\n",
       "      <td>114.940002</td>\n",
       "      <td>20858700.0</td>\n",
       "      <td>0.176267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>Credit Card &amp; BNPL</td>\n",
       "      <td>AFFIRM HOLDINGS INC</td>\n",
       "      <td>AFRM</td>\n",
       "      <td>123.054001</td>\n",
       "      <td>127.239998</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>8310700.0</td>\n",
       "      <td>0.017764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-19</td>\n",
       "      <td>Credit Card &amp; BNPL</td>\n",
       "      <td>AFFIRM HOLDINGS INC</td>\n",
       "      <td>AFRM</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.675003</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>110.989998</td>\n",
       "      <td>110.989998</td>\n",
       "      <td>3304900.0</td>\n",
       "      <td>-0.052734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>Credit Card &amp; BNPL</td>\n",
       "      <td>AFFIRM HOLDINGS INC</td>\n",
       "      <td>AFRM</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>103.250000</td>\n",
       "      <td>106.209999</td>\n",
       "      <td>106.209999</td>\n",
       "      <td>5091400.0</td>\n",
       "      <td>-0.044022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25993</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>Fintech Hardware</td>\n",
       "      <td>SQUARE (BLOCK INC)</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>73.385002</td>\n",
       "      <td>73.830002</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>73.669998</td>\n",
       "      <td>73.669998</td>\n",
       "      <td>6578200.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25994</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "      <td>BLOCK INC</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>74.449997</td>\n",
       "      <td>76.154999</td>\n",
       "      <td>74.334999</td>\n",
       "      <td>75.389999</td>\n",
       "      <td>75.389999</td>\n",
       "      <td>7237200.0</td>\n",
       "      <td>0.023079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25995</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Fintech Hardware</td>\n",
       "      <td>SQUARE (BLOCK INC)</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>74.449997</td>\n",
       "      <td>76.154999</td>\n",
       "      <td>74.334999</td>\n",
       "      <td>75.389999</td>\n",
       "      <td>75.389999</td>\n",
       "      <td>7237200.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25996</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "      <td>BLOCK INC</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>75.320000</td>\n",
       "      <td>75.665001</td>\n",
       "      <td>71.610001</td>\n",
       "      <td>72.269997</td>\n",
       "      <td>72.269997</td>\n",
       "      <td>7838100.0</td>\n",
       "      <td>-0.042266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25997</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>Fintech Hardware</td>\n",
       "      <td>SQUARE (BLOCK INC)</td>\n",
       "      <td>XYZ</td>\n",
       "      <td>75.320000</td>\n",
       "      <td>75.665001</td>\n",
       "      <td>71.610001</td>\n",
       "      <td>72.269997</td>\n",
       "      <td>72.269997</td>\n",
       "      <td>7838100.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25998 rows Ã— 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T00:28:59.046144Z",
     "start_time": "2025-10-03T00:28:55.776837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "FinTech/ç›¸å…³è‚¡ç¥¨ - K-Means èšç±»ï¼ˆå« Nameâ†’Ticker æ˜ å°„ã€æ•°æ®æŠ“å–ã€ç‰¹å¾å·¥ç¨‹ã€è‡ªåŠ¨é€‰Kã€å¯¼å‡ºç»“æœï¼‰\n",
    "Author: You + ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NAME_TO_TICKER: Dict[str, Optional[str]] = {\n",
    "    \"BLOCK INC\": \"SQ\",\n",
    "    \"PAYPAL HOLDINGS INC\": \"PYPL\",\n",
    "    \"SHIFT4 PAYMENTS INC-CLASS A\": \"FOUR\",\n",
    "    \"STONECO LTD-A\": \"STNE\",\n",
    "    \"AFFIRM HOLDINGS INC\": \"AFRM\",\n",
    "    \"BILL HOLDINGS INC\": \"BILL\",\n",
    "    \"FIDELITY NATIONAL INFO SERV\": \"FIS\",\n",
    "    \"FISERV INC\": \"FI\",\n",
    "    \"JACK HENRY & ASSOCIATES INC\": \"JKHY\",\n",
    "    \"LEMONADE INC\": \"LMND\",\n",
    "    \"NCINO INC\": \"NCNO\",\n",
    "    \"PAGSEGURO DIGITAL LTD-CL A\": \"PAGS\",\n",
    "    \"SOFI TECHNOLOGIES INC\": \"SOFI\",\n",
    "    \"TOAST INC-CLASS A\": \"TOST\",\n",
    "    \"ACI WORLDWIDE INC\": \"ACIW\",\n",
    "    \"BLACKLINE INC\": \"BL\",\n",
    "    \"BROADRIDGE FINANCIAL SOLUTIO\": \"BR\",\n",
    "    \"COINBASE GLOBAL INC -CLASS A\": \"COIN\",\n",
    "    \"DLOCAL LTD\": \"DLO\",\n",
    "    \"FLYWIRE CORP-VOTING\": \"FLYW\",\n",
    "    \"GLOBAL PAYMENTS INC\": \"GPN\",\n",
    "    \"GUIDEWIRE SOFTWARE INC\": \"GWRE\",\n",
    "    \"INTUIT INC\": \"INTU\",\n",
    "    \"MARQETA INC-A\": \"MQ\",\n",
    "    \"Q2 HOLDINGS INC\": \"QTWO\",\n",
    "    \"SS&C TECHNOLOGIES HOLDINGS\": \"SSNC\",\n",
    "    \"UPSTART HOLDINGS INC\": \"UPST\",\n",
    "    \"VERISK ANALYTICS INC\": \"VRSK\",\n",
    "    \"WEX INC\": \"WEX\",\n",
    "    \"WORLDLINE SA\": \"WLN.PA\",\n",
    "    \"AMERICAN EXPRESS CO\": \"AXP\",\n",
    "    \"AVIDXCHANGE HOLDINGS INC\": \"AVDX\",\n",
    "    \"BLEND LABS INC-A\": \"BLND\",\n",
    "    \"BREAD FINANCIAL HOLDINGS INC\": \"BFH\",\n",
    "    \"EURONET WORLDWIDE INC\": \"EEFT\",\n",
    "    \"GREEN DOT CORP-CLASS A\": \"GDOT\",\n",
    "    \"LENDINGTREE INC\": \"TREE\",\n",
    "    \"MARKETAXESS HOLDINGS INC\": \"MKTX\",\n",
    "    \"MASTERCARD INC - A\": \"MA\",\n",
    "    \"NCR VOYIX CORP\": \"VYX\",\n",
    "    \"NEXI SPA\": \"NEXI.MI\",\n",
    "    \"NU HOLDINGS LTD/CAYMAN ISL-A\": \"NU\",\n",
    "    \"ONE 97 COMMUNICATIONS LTD\": \"PAYTM.NS\",\n",
    "    \"PAGAYA TECHNOLOGIES LTD -A\": \"PGY\",\n",
    "    \"PAYMENTUS HOLDINGS INC-A\": \"PAY\",\n",
    "    \"PAYONEER GLOBAL INC\": \"PAYO\",\n",
    "    \"QFIN HOLDINGS INC-ADR\": \"QFIN\",  # 360 DigiTech\n",
    "    \"REPAY HOLDINGS CORP\": \"RPAY\",\n",
    "    \"SBI CARDS & PAYMENT SERVICES\": \"SBICARD.NS\",\n",
    "    \"TEMENOS AG - REG\": \"TEMN.SW\",\n",
    "    \"TRADEWEB MARKETS INC-CLASS A\": \"TW\",\n",
    "    \"VISA INC-CLASS A SHARES\": \"V\",\n",
    "    \"WESTERN UNION CO\": \"WU\",\n",
    "    \"XERO LTD\": \"XRO.AX\",\n",
    "    \"ALIBABA GROUP HOLDING LTD\": \"BABA\",\n",
    "    \"ALKAMI TECHNOLOGY INC\": \"ALKT\",\n",
    "    \"AMAZON.COM INC\": \"AMZN\",\n",
    "    \"APPLE INC\": \"AAPL\",\n",
    "    \"CANTALOUPE INC\": \"CTLP\",\n",
    "    \"COSTAR GROUP INC\": \"CSGP\",\n",
    "    \"DEUTSCHE BOERSE AG\": \"DB1.DE\",\n",
    "    \"DOUBLEVERIFY HOLDINGS INC\": \"DV\",\n",
    "    \"EDENRED\": \"EDEN.PA\",\n",
    "    \"EVERTEC INC\": \"EVTC\",\n",
    "    \"EXPENSIFY INC - A\": \"EXFY\",\n",
    "    \"EXPERIAN PLC\": \"EXPN.L\",\n",
    "    \"FACTSET RESEARCH SYSTEMS INC\": \"FDS\",\n",
    "    \"FAIR ISAAC CORP\": \"FICO\",\n",
    "    \"FINVOLUTION GROUP-ADR\": \"FINV\",\n",
    "    \"GLOBAL-E ONLINE LTD\": \"GLBE\",\n",
    "    \"GMO PAYMENT GATEWAY INC\": \"3769.T\",\n",
    "    \"GOTO GOJEK TOKOPEDIA TBK PT\": \"GOTO.JK\",\n",
    "    \"GRAB HOLDINGS LTD - CL A\": \"GRAB\",\n",
    "    \"HUB24 LTD\": \"HUB.AX\",\n",
    "    \"HYPOPORT SE\": \"HYQ.DE\",\n",
    "    \"I3 VERTICALS INC-CLASS A\": \"IIIV\",\n",
    "    \"INTAPP INC\": \"INTA\",\n",
    "    \"INTERCONTINENTAL EXCHANGE IN\": \"ICE\",\n",
    "    \"INTERNATIONAL MONEY EXPRESS\": \"IMXI\",\n",
    "    \"JSC KASPI.KZ ADR\": \"KSPI\",\n",
    "    \"KAKAOPAY CORP\": \"377300.KS\",\n",
    "    \"LENDINGCLUB CORP\": \"LC\",\n",
    "    \"LEXINFINTECH HOLDINGS L-ADR\": \"LX\",\n",
    "    \"LIGHTSPEED COMMERCE INC\": \"LSPD\",\n",
    "    \"LPL FINANCIAL HOLDINGS INC\": \"LPLA\",\n",
    "    \"MERCADOLIBRE INC\": \"MELI\",\n",
    "    \"MERIDIANLINK INC\": \"MLNK\",\n",
    "    \"META PLATFORMS INC-CLASS A\": \"META\",\n",
    "    \"MITEK SYSTEMS INC\": \"MITK\",\n",
    "    \"NASDAQ INC\": \"NDAQ\",\n",
    "    \"NAVER CORP\": \"035420.KS\",\n",
    "    \"ONESPAN INC\": \"OSPN\",\n",
    "    \"OPEN LENDING CORP\": \"LPRO\",\n",
    "    \"PAX GLOBAL TECHNOLOGY LTD\": \"AZI\",\n",
    "    \"PAYPOINT PLC\": \"PAY.L\",\n",
    "    \"PAYSAFE LTD\": \"PSFE\",\n",
    "    \"PEGASYSTEMS INC\": \"PEGA\",\n",
    "    \"PRIORITY TECHNOLOGY HOLDINGS\": \"PRTH\",\n",
    "    \"RAKUTEN BANK LTD\": \"5838.T\",\n",
    "    \"REMITLY GLOBAL INC\": \"RELY\",\n",
    "    \"RISKIFIED LTD-A\": \"RSKD\",\n",
    "    \"ROBINHOOD MARKETS INC - A\": \"HOOD\",\n",
    "    \"SAGE GROUP PLC/THE\": \"SGE.L\",\n",
    "    \"SEA LTD-ADR\": \"SE\",\n",
    "    \"SEI INVESTMENTS COMPANY\": \"SEIC\",\n",
    "    \"SEZZLE INC\": \"SEZL\",\n",
    "    \"SHOPIFY INC - CLASS A\": \"SHOP\",\n",
    "    \"SYNCHRONY FINANCIAL\": \"SYF\",\n",
    "    \"TENCENT HOLDINGS LTD\": \"0700.HK\",\n",
    "    \"WALMART INC\": \"WMT\",\n",
    "    \"WIX.COM LTD\": \"WIX\",\n",
    "    \"YEAHKA LTD\": \"9923.HK\",\n",
    "    \"ZILLOW GROUP INC - C\": \"Z\",\n",
    "    \"ZIP CO LTD\": \"ZIP.AX\",\n",
    "    \"CAPITAL ONE FINANCIAL CORP\": \"COF\",\n",
    "    \"CCC INTELLIGENT SOLUTIONS HO\": \"CCCS\",\n",
    "    \"CIELO SA\": \"CIEL3.SA\",\n",
    "    \"CIPHER MINING INC\": \"CIFR\",\n",
    "    \"CLEANSPARK INC\": \"CLSK\",\n",
    "    \"DISCOVERY LTD\": \"DSY.JO\",\n",
    "    \"DOCUSIGN INC\": \"DOCU\",\n",
    "    \"EML PAYMENTS LTD\": \"EML.AX\",\n",
    "    \"EPAM SYSTEMS INC\": \"EPAM\",\n",
    "    \"EQUIFAX INC\": \"EFX\",\n",
    "    \"EVERQUOTE INC - CLASS A\": \"EVER\",\n",
    "    \"FUTU HOLDINGS LTD-ADR\": \"FUTU\",\n",
    "    \"HDFC BANK LIMITED\": \"HDB\",\n",
    "    \"INTERACTIVE BROKERS GRO-CL A\": \"IBKR\",\n",
    "    \"IRESS LTD\": \"IRE.AX\",\n",
    "    \"JPMORGAN CHASE & CO\": \"JPM\",\n",
    "    \"LINKLOGIS INC-CLASS B\": \"9959.HK\",\n",
    "    \"LUFAX HOLDING LTD-ADR\": \"LU\",\n",
    "    \"MARA HOLDINGS INC\": \"MARA\",\n",
    "    \"MOODY'S CORP\": \"MCO\",\n",
    "    \"MSCI INC\": \"MSCI\",\n",
    "    \"ONESTREAM INC\": \"ONES\",\n",
    "    \"PDD HOLDINGS INC\": \"PDD\",\n",
    "    \"QUDIAN INC-SPON ADR\": \"QD\",\n",
    "    \"RIOT PLATFORMS INC\": \"RIOT\",\n",
    "    \"S&P GLOBAL INC\": \"SPGI\",\n",
    "    \"SAPIENS INTERNATIONAL CORP\": \"SPNS\",\n",
    "    \"STRATEGY INC\": \"MSTR\",\n",
    "    \"THOMSON REUTERS CORP\": \"TRI\",\n",
    "    \"UP FINTECH HOLDING LTD - ADR\": \"TIGR\",\n",
    "    \"VIRTU FINANCIAL INC-CLASS A\": \"VIRT\",\n",
    "    \"WORKDAY INC-CLASS A\": \"WDAY\",\n",
    "    \"WORKIVA INC\": \"WK\",\n",
    "    \"ZSCALER INC\": \"ZS\",\n",
    "}\n",
    "\n",
    "# ========= 2) æŠŠ Name â†’ Ticker è½¬æˆ DataFrame =========\n",
    "name_ticker_df = pd.DataFrame(\n",
    "    [{\"Name\": name, \"Ticker\": tick} for name, tick in NAME_TO_TICKER.items()]\n",
    ")\n",
    "print(f\"Total names: {len(name_ticker_df)} ; with ticker: {name_ticker_df['Ticker'].notna().sum()}\")\n",
    "\n",
    "# ========= 3) æŠ“å–ä»·æ ¼ä¸ä¿¡æ¯ï¼Œæ„é€ ç‰¹å¾ =========\n",
    "# é€‰å–çš„ç‰¹å¾ï¼š\n",
    "# - å¸‚å€¼ï¼ˆå–å¯¹æ•°ï¼šlog_mktcapï¼‰\n",
    "# - PE (trailing PE)\n",
    "# - PB (price-to-book)\n",
    "# - å¹´åŒ–æ³¢åŠ¨ç‡ï¼ˆè¿‡å» ~2 å¹´æ—¥é¢‘æ”¶ç›˜æ”¶ç›Šç‡ * sqrt(252)ï¼‰\n",
    "# - Betaï¼ˆç›¸å¯¹æ ‡æ™®500 ^GSPCï¼Œæ—¥é¢‘å›å½’ï¼‰\n",
    "#\n",
    "\n",
    "def fetch_features(ticker: str, benchmark_hist: pd.DataFrame) -> Optional[dict]:\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        # info é‡Œå­—æ®µåœ¨ä¸åŒå¸‚åœºå¯èƒ½ç¼ºå¤±ï¼Œå°½é‡å…¼å®¹\n",
    "        info = t.info if hasattr(t, \"info\") else {}\n",
    "        mktcap = info.get(\"marketCap\", np.nan)\n",
    "        pe = info.get(\"trailingPE\", np.nan)\n",
    "        pb = info.get(\"priceToBook\", np.nan)\n",
    "\n",
    "        # å†å²ä»·æ ¼ï¼šä¸¤å¹´è¶³å¤Ÿ\n",
    "        hist = t.history(period=\"2y\", interval=\"1d\", auto_adjust=True)\n",
    "        if hist is None or hist.empty or \"Close\" not in hist.columns:\n",
    "            return None\n",
    "\n",
    "        # è®¡ç®—æ—¥æ”¶ç›Šã€æ³¢åŠ¨ç‡\n",
    "        ret = hist[\"Close\"].pct_change().dropna()\n",
    "        if ret.size < 60:\n",
    "            return None\n",
    "        vol_ann = float(ret.std() * np.sqrt(252))\n",
    "\n",
    "        # è®¡ç®— Betaï¼ˆä¸ ^GSPC å¯¹é½ï¼‰\n",
    "        bench = benchmark_hist[\"Close\"].pct_change().dropna()\n",
    "        df = pd.merge(\n",
    "            ret.to_frame(\"ret\"),\n",
    "            bench.to_frame(\"mkt\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        if df.shape[0] < 60 or df[\"mkt\"].std() == 0:\n",
    "            beta = np.nan\n",
    "        else:\n",
    "            cov = np.cov(df[\"ret\"], df[\"mkt\"])[0, 1]\n",
    "            beta = float(cov / (df[\"mkt\"].var()))\n",
    "\n",
    "        log_mktcap = np.nan\n",
    "        if pd.notna(mktcap) and mktcap > 0:\n",
    "            log_mktcap = float(np.log(mktcap))\n",
    "\n",
    "        return {\n",
    "            \"Ticker\": ticker,\n",
    "            \"market_cap\": mktcap,\n",
    "            \"log_mktcap\": log_mktcap,\n",
    "            \"pe_trailing\": pe,\n",
    "            \"pb\": pb,\n",
    "            \"vol_ann\": vol_ann,\n",
    "            \"beta\": beta,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# åŸºå‡†æŒ‡æ•°ï¼ˆæ ‡æ™®500ï¼‰ï¼›å¦‚å¤§å¤šä¸ºéç¾è‚¡ï¼Œå¯æ”¹ä¸º â€œ^STOXX50Eâ€/â€œ^HSIâ€ç­‰\n",
    "benchmark = yf.Ticker(\"^GSPC\").history(period=\"2y\", interval=\"1d\", auto_adjust=True)\n",
    "\n",
    "rows = []\n",
    "TODO_names = []\n",
    "for _, row in name_ticker_df.iterrows():\n",
    "    name = row[\"Name\"]\n",
    "    tick = row[\"Ticker\"]\n",
    "    if pd.isna(tick) or not isinstance(tick, str) or not tick.strip():\n",
    "        TODO_names.append(name)\n",
    "        continue\n",
    "    feats = fetch_features(tick.strip(), benchmark)\n",
    "    if feats is not None:\n",
    "        feats[\"Name\"] = name\n",
    "        rows.append(feats)\n",
    "    # å°æ†©é˜²æ­¢é¢‘ç‡è¿‡é«˜\n",
    "    time.sleep(0.15)\n",
    "\n",
    "raw_feat_df = pd.DataFrame(rows)\n",
    "print(f\"Fetched features for: {raw_feat_df.shape[0]} tickers\")\n",
    "if TODO_names:\n",
    "    print(\"\\n[TODO] è¿™äº›å…¬å¸ç¼ºå°‘Tickeræˆ–éœ€ç¡®è®¤åç¼€ï¼Œè¯·åœ¨ NAME_TO_TICKER ä¸­è¡¥å……ï¼š\")\n",
    "    for nm in TODO_names:\n",
    "        print(\" -\", nm)\n",
    "\n",
    "# ========= 4) ç‰¹å¾å·¥ç¨‹ & ç¼ºå¤±å€¼å¤„ç† =========\n",
    "# é€‰æ‹©ç”¨äºèšç±»çš„ç‰¹å¾åˆ—\n",
    "feature_cols = [\"log_mktcap\", \"pe_trailing\", \"pb\", \"vol_ann\", \"beta\"]\n",
    "\n",
    "feat_df = raw_feat_df.copy()\n",
    "X = feat_df[feature_cols].copy()\n",
    "\n",
    "# ç¼ºå¤±å€¼ç”¨ä¸­ä½æ•°å¡«è¡¥\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# ========= 5) è‡ªåŠ¨é€‰æ‹© K =========\n",
    "# ä½¿ç”¨ 2..8 çš„ Kï¼ŒæŒ‘é€‰ silhouette score æœ€é«˜çš„Kï¼ˆè‹¥æ ·æœ¬è¿‡å°åˆ™é€€åŒ–ä¸ºå›ºå®šKï¼‰\n",
    "best_k, best_score = None, -1\n",
    "k_candidates = [2, 3, 4, 5, 6, 7, 8]\n",
    "n_samples = X_scaled.shape[0]\n",
    "\n",
    "if n_samples >= 10:\n",
    "    for k in k_candidates:\n",
    "        if k >= n_samples:  # è‡³å°‘æ¯ç°‡è¦æœ‰ç‚¹\n",
    "            continue\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "        labels = km.fit_predict(X_scaled)\n",
    "        try:\n",
    "            score = silhouette_score(X_scaled, labels)\n",
    "        except Exception:\n",
    "            score = -1\n",
    "        if score > best_score:\n",
    "            best_k, best_score = k, score\n",
    "else:\n",
    "    # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œé€€è€Œæ±‚å…¶æ¬¡\n",
    "    best_k = min(3, max(2, n_samples - 1))\n",
    "\n",
    "print(f\"\\n[Info] Auto-selected K = {best_k} (silhouette={best_score:.4f} ; samples={n_samples})\")\n",
    "\n",
    "# ========= 6) æœ€ç»ˆ K-Means èšç±» =========\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=50, random_state=42)\n",
    "feat_df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# ========= 7) ç»“æœæ±‡æ€» & å¯¼å‡º =========\n",
    "# æ¯ä¸ªç°‡çš„ç‰¹å¾å‡å€¼ï¼ˆåæ ‡å‡†åŒ–æ„ä¹‰æœ‰é™ï¼Œè¿™é‡Œç”¨åŸå§‹æŒ‡æ ‡çš„å¹³å‡å€¼ä»…ä½œå‚è€ƒï¼‰\n",
    "summary = (\n",
    "    feat_df.groupby(\"Cluster\")[feature_cols]\n",
    "    .mean(numeric_only=True)\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ç”Ÿæˆç°‡å†…æ¸…å•\n",
    "cluster_lists = (\n",
    "    feat_df.sort_values([\"Cluster\", \"Name\"])\n",
    "    .groupby(\"Cluster\")\n",
    "    .apply(lambda g: \", \".join(g[\"Name\"].tolist()))\n",
    "    .to_frame(\"NamesInCluster\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# åˆå¹¶æ‘˜è¦\n",
    "summary = pd.merge(summary, cluster_lists, on=\"Cluster\", how=\"left\")\n",
    "\n",
    "# ä¸ Name/Ticker åˆå¹¶ï¼Œè¾“å‡ºæ˜ç»†\n",
    "detail_cols = [\"Name\", \"Ticker\", \"market_cap\", \"pe_trailing\", \"pb\", \"vol_ann\", \"beta\", \"Cluster\"]\n",
    "details = feat_df[detail_cols].sort_values([\"Cluster\", \"Name\"]).reset_index(drop=True)\n",
    "\n",
    "# å¯¼å‡º\n",
    "summary_file = \"cluster_summary.xlsx\"\n",
    "with pd.ExcelWriter(summary_file, engine=\"openpyxl\") as writer:\n",
    "    details.to_excel(writer, sheet_name=\"Details\", index=False)\n",
    "    summary.to_excel(writer, sheet_name=\"ClusterSummary\", index=False)\n",
    "    name_ticker_df.to_excel(writer, sheet_name=\"NameTickerMap\", index=False)\n",
    "\n",
    "details.to_csv(\"peer_groups_details.csv\", index=False, encoding=\"utf-8\")\n",
    "summary.to_csv(\"peer_groups_summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[OK] å¯¼å‡ºå®Œæˆï¼š\\n - {summary_file}\\n - peer_groups_details.csv\\n - peer_groups_summary.csv\")\n",
    "\n",
    "# ========= 8) å‹å¥½æ‰“å° =========\n",
    "def fmt(x):\n",
    "    try:\n",
    "        if pd.isna(x): return \"NA\"\n",
    "        return f\"{x:,.2f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "print(\"\\n=== Cluster Summary (å‡å€¼/å‚è€ƒ) ===\")\n",
    "print(summary[[\"Cluster\"] + feature_cols + [\"NamesInCluster\"]])\n",
    "\n",
    "print(\"\\n=== æ¯ä¸ªç°‡çš„å…¬å¸åˆ—è¡¨ ===\")\n",
    "for c, g in feat_df.sort_values([\"Cluster\", \"Name\"]).groupby(\"Cluster\"):\n",
    "    print(f\"\\nCluster {c}:\")\n",
    "    for _, r in g.iterrows():\n",
    "        print(f\" - {r['Name']} ({r['Ticker']}) | PE={fmt(r['pe_trailing'])}, PB={fmt(r['pb'])}, \"\n",
    "              f\"Vol={fmt(r['vol_ann'])}, Beta={fmt(r['beta'])}\")\n"
   ],
   "id": "5381b163db2b923d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 160 ; with ticker: 157\n"
     ]
    },
    {
     "ename": "YFRateLimitError",
     "evalue": "Too Many Requests. Rate limited. Try after a while.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mYFRateLimitError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 260\u001B[39m\n\u001B[32m    256\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# åŸºå‡†æŒ‡æ•°ï¼ˆæ ‡æ™®500ï¼‰ï¼›å¦‚å¤§å¤šä¸ºéç¾è‚¡ï¼Œå¯æ”¹ä¸º â€œ^STOXX50Eâ€/â€œ^HSIâ€ç­‰\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m260\u001B[39m benchmark = \u001B[43myf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTicker\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m^GSPC\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mperiod\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m2y\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterval\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m1d\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauto_adjust\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    262\u001B[39m rows = []\n\u001B[32m    263\u001B[39m TODO_names = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\utils.py:103\u001B[39m, in \u001B[36mlog_indent_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    100\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\base.py:91\u001B[39m, in \u001B[36mTickerBase.history\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[38;5;129m@utils\u001B[39m.log_indent_decorator\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhistory\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs) -> pd.DataFrame:\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lazy_load_price_history\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.history(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\base.py:97\u001B[39m, in \u001B[36mTickerBase._lazy_load_price_history\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_lazy_load_price_history\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._price_history \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m         \u001B[38;5;28mself\u001B[39m._price_history = PriceHistory(\u001B[38;5;28mself\u001B[39m._data, \u001B[38;5;28mself\u001B[39m.ticker, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_ticker_tz\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m     98\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._price_history\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\base.py:112\u001B[39m, in \u001B[36mTickerBase._get_ticker_tz\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    109\u001B[39m     tz = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tz \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m     tz = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fetch_ticker_tz\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    113\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m tz \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    114\u001B[39m         \u001B[38;5;66;03m# _fetch_ticker_tz works in 99.999% of cases.\u001B[39;00m\n\u001B[32m    115\u001B[39m         \u001B[38;5;66;03m# For rare fail get from info.\u001B[39;00m\n\u001B[32m    116\u001B[39m         \u001B[38;5;28;01mglobal\u001B[39;00m _tz_info_fetch_ctr\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\utils.py:103\u001B[39m, in \u001B[36mlog_indent_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    100\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\base.py:144\u001B[39m, in \u001B[36mTickerBase._fetch_ticker_tz\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    141\u001B[39m url = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_BASE_URL_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/v8/finance/chart/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.ticker\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    143\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m144\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcache_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    145\u001B[39m     data = data.json()\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m YFRateLimitError:\n\u001B[32m    147\u001B[39m     \u001B[38;5;66;03m# Must propagate this\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\data.py:32\u001B[39m, in \u001B[36mlru_cache_freezeargs.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     30\u001B[39m args = \u001B[38;5;28mtuple\u001B[39m([\u001B[38;5;28mtuple\u001B[39m(arg) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args])\n\u001B[32m     31\u001B[39m kwargs = {k: \u001B[38;5;28mtuple\u001B[39m(v) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs.items()}\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\data.py:431\u001B[39m, in \u001B[36mYfData.cache_get\u001B[39m\u001B[34m(self, url, user_agent_headers, params, timeout)\u001B[39m\n\u001B[32m    428\u001B[39m \u001B[38;5;129m@lru_cache_freezeargs\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;129m@lru_cache\u001B[39m(maxsize=cache_maxsize)\n\u001B[32m    430\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcache_get\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, user_agent_headers=\u001B[38;5;28;01mNone\u001B[39;00m, params=\u001B[38;5;28;01mNone\u001B[39;00m, timeout=\u001B[32m30\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\utils.py:103\u001B[39m, in \u001B[36mlog_indent_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    100\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\data.py:364\u001B[39m, in \u001B[36mYfData.get\u001B[39m\u001B[34m(self, url, user_agent_headers, params, timeout)\u001B[39m\n\u001B[32m    362\u001B[39m \u001B[38;5;129m@utils\u001B[39m.log_indent_decorator\n\u001B[32m    363\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, user_agent_headers=\u001B[38;5;28;01mNone\u001B[39;00m, params=\u001B[38;5;28;01mNone\u001B[39;00m, timeout=\u001B[32m30\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_method\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_session\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\utils.py:103\u001B[39m, in \u001B[36mlog_indent_decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    100\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEntering \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m IndentationContext():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    105\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mExiting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m()\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\Horisation\\Lib\\site-packages\\yfinance\\data.py:424\u001B[39m, in \u001B[36mYfData._make_request\u001B[39m\u001B[34m(self, url, request_method, user_agent_headers, body, params, timeout)\u001B[39m\n\u001B[32m    422\u001B[39m     \u001B[38;5;66;03m# Raise exception if rate limited\u001B[39;00m\n\u001B[32m    423\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m response.status_code == \u001B[32m429\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m424\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m YFRateLimitError()\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[31mYFRateLimitError\u001B[39m: Too Many Requests. Rate limited. Try after a while."
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T00:42:08.363935Z",
     "start_time": "2025-10-03T00:41:44.625345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Peer Group via K-Means (using your NAME_TO_TICKER mapping)\n",
    "Features from yfinance Ticker.info (valuation, profitability, health, growth)\n",
    "Exports: cluster_summary.xlsx + CSVs\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============== 1) ä½¿ç”¨ä½ æä¾›çš„ Name -> Ticker æ˜ å°„ / Your mapping ==============\n",
    "NAME_TO_TICKER: Dict[str, Optional[str]] = {\n",
    "    \"BLOCK INC\": \"SQ\",\n",
    "    \"PAYPAL HOLDINGS INC\": \"PYPL\",\n",
    "    \"SHIFT4 PAYMENTS INC-CLASS A\": \"FOUR\",\n",
    "    \"STONECO LTD-A\": \"STNE\",\n",
    "    \"AFFIRM HOLDINGS INC\": \"AFRM\",\n",
    "    \"BILL HOLDINGS INC\": \"BILL\",\n",
    "    \"FIDELITY NATIONAL INFO SERV\": \"FIS\",\n",
    "    \"FISERV INC\": \"FI\",\n",
    "    \"JACK HENRY & ASSOCIATES INC\": \"JKHY\",\n",
    "    \"LEMONADE INC\": \"LMND\",\n",
    "    \"NCINO INC\": \"NCNO\",\n",
    "    \"PAGSEGURO DIGITAL LTD-CL A\": \"PAGS\",\n",
    "    \"SOFI TECHNOLOGIES INC\": \"SOFI\",\n",
    "    \"TOAST INC-CLASS A\": \"TOST\",\n",
    "    \"ACI WORLDWIDE INC\": \"ACIW\",\n",
    "    \"BLACKLINE INC\": \"BL\",\n",
    "    \"BROADRIDGE FINANCIAL SOLUTIO\": \"BR\",\n",
    "    \"COINBASE GLOBAL INC -CLASS A\": \"COIN\",\n",
    "    \"DLOCAL LTD\": \"DLO\",\n",
    "    \"FLYWIRE CORP-VOTING\": \"FLYW\",\n",
    "    \"GLOBAL PAYMENTS INC\": \"GPN\",\n",
    "    \"GUIDEWIRE SOFTWARE INC\": \"GWRE\",\n",
    "    \"INTUIT INC\": \"INTU\",\n",
    "    \"MARQETA INC-A\": \"MQ\",\n",
    "    \"Q2 HOLDINGS INC\": \"QTWO\",\n",
    "    \"SS&C TECHNOLOGIES HOLDINGS\": \"SSNC\",\n",
    "    \"UPSTART HOLDINGS INC\": \"UPST\",\n",
    "    \"VERISK ANALYTICS INC\": \"VRSK\",\n",
    "    \"WEX INC\": \"WEX\",\n",
    "    \"WORLDLINE SA\": \"WLN.PA\",\n",
    "    \"AMERICAN EXPRESS CO\": \"AXP\",\n",
    "    \"AVIDXCHANGE HOLDINGS INC\": \"AVDX\",\n",
    "    \"BLEND LABS INC-A\": \"BLND\",\n",
    "    \"BREAD FINANCIAL HOLDINGS INC\": \"BFH\",\n",
    "    \"EURONET WORLDWIDE INC\": \"EEFT\",\n",
    "    \"GREEN DOT CORP-CLASS A\": \"GDOT\",\n",
    "    \"LENDINGTREE INC\": \"TREE\",\n",
    "    \"MARKETAXESS HOLDINGS INC\": \"MKTX\",\n",
    "    \"MASTERCARD INC - A\": \"MA\",\n",
    "    \"NCR VOYIX CORP\": \"VYX\",\n",
    "    \"NEXI SPA\": \"NEXI.MI\",\n",
    "    \"NU HOLDINGS LTD/CAYMAN ISL-A\": \"NU\",\n",
    "    \"ONE 97 COMMUNICATIONS LTD\": \"PAYTM.NS\",\n",
    "    \"PAGAYA TECHNOLOGIES LTD -A\": \"PGY\",\n",
    "    \"PAYMENTUS HOLDINGS INC-A\": \"PAY\",\n",
    "    \"PAYONEER GLOBAL INC\": \"PAYO\",\n",
    "    \"QFIN HOLDINGS INC-ADR\": \"QFIN\",\n",
    "    \"REPAY HOLDINGS CORP\": \"RPAY\",\n",
    "    \"SBI CARDS & PAYMENT SERVICES\": \"SBICARD.NS\",\n",
    "    \"TEMENOS AG - REG\": \"TEMN.SW\",\n",
    "    \"TRADEWEB MARKETS INC-CLASS A\": \"TW\",\n",
    "    \"VISA INC-CLASS A SHARES\": \"V\",\n",
    "    \"WESTERN UNION CO\": \"WU\",\n",
    "    \"XERO LTD\": \"XRO.AX\",\n",
    "    \"ALIBABA GROUP HOLDING LTD\": \"BABA\",\n",
    "    \"ALKAMI TECHNOLOGY INC\": \"ALKT\",\n",
    "    \"AMAZON.COM INC\": \"AMZN\",\n",
    "    \"APPLE INC\": \"AAPL\",\n",
    "    \"CANTALOUPE INC\": \"CTLP\",\n",
    "    \"COSTAR GROUP INC\": \"CSGP\",\n",
    "    \"DEUTSCHE BOERSE AG\": \"DB1.DE\",\n",
    "    \"DOUBLEVERIFY HOLDINGS INC\": \"DV\",\n",
    "    \"EDENRED\": \"EDEN.PA\",\n",
    "    \"EVERTEC INC\": \"EVTC\",\n",
    "    \"EXPENSIFY INC - A\": \"EXFY\",\n",
    "    \"EXPERIAN PLC\": \"EXPN.L\",\n",
    "    \"FACTSET RESEARCH SYSTEMS INC\": \"FDS\",\n",
    "    \"FAIR ISAAC CORP\": \"FICO\",\n",
    "    \"FINVOLUTION GROUP-ADR\": \"FINV\",\n",
    "    \"GLOBAL-E ONLINE LTD\": \"GLBE\",\n",
    "    \"GMO PAYMENT GATEWAY INC\": \"3769.T\",\n",
    "    \"GOTO GOJEK TOKOPEDIA TBK PT\": \"GOTO.JK\",\n",
    "    \"GRAB HOLDINGS LTD - CL A\": \"GRAB\",\n",
    "    \"HUB24 LTD\": \"HUB.AX\",\n",
    "    \"HYPOPORT SE\": \"HYQ.DE\",\n",
    "    \"I3 VERTICALS INC-CLASS A\": \"IIIV\",\n",
    "    \"INTAPP INC\": \"INTA\",\n",
    "    \"INTERCONTINENTAL EXCHANGE IN\": \"ICE\",\n",
    "    \"INTERNATIONAL MONEY EXPRESS\": \"IMXI\",\n",
    "    \"JSC KASPI.KZ ADR\": \"KSPI\",\n",
    "    \"KAKAOPAY CORP\": \"377300.KS\",\n",
    "    \"LENDINGCLUB CORP\": \"LC\",\n",
    "    \"LEXINFINTECH HOLDINGS L-ADR\": \"LX\",\n",
    "    \"LIGHTSPEED COMMERCE INC\": \"LSPD\",\n",
    "    \"LPL FINANCIAL HOLDINGS INC\": \"LPLA\",\n",
    "    \"MERCADOLIBRE INC\": \"MELI\",\n",
    "    \"MERIDIANLINK INC\": \"MLNK\",\n",
    "    \"META PLATFORMS INC-CLASS A\": \"META\",\n",
    "    \"MITEK SYSTEMS INC\": \"MITK\",\n",
    "    \"NASDAQ INC\": \"NDAQ\",\n",
    "    \"NAVER CORP\": \"035420.KS\",\n",
    "    \"ONESPAN INC\": \"OSPN\",\n",
    "    \"OPEN LENDING CORP\": \"LPRO\",\n",
    "    \"PAX GLOBAL TECHNOLOGY LTD\": \"AZI\",\n",
    "    \"PAYPOINT PLC\": \"PAY.L\",\n",
    "    \"PAYSAFE LTD\": \"PSFE\",\n",
    "    \"PEGASYSTEMS INC\": \"PEGA\",\n",
    "    \"PRIORITY TECHNOLOGY HOLDINGS\": \"PRTH\",\n",
    "    \"RAKUTEN BANK LTD\": \"5838.T\",\n",
    "    \"REMITLY GLOBAL INC\": \"RELY\",\n",
    "    \"RISKIFIED LTD-A\": \"RSKD\",\n",
    "    \"ROBINHOOD MARKETS INC - A\": \"HOOD\",\n",
    "    \"SAGE GROUP PLC/THE\": \"SGE.L\",\n",
    "    \"SEA LTD-ADR\": \"SE\",\n",
    "    \"SEI INVESTMENTS COMPANY\": \"SEIC\",\n",
    "    \"SEZZLE INC\": \"SEZL\",\n",
    "    \"SHOPIFY INC - CLASS A\": \"SHOP\",\n",
    "    \"SYNCHRONY FINANCIAL\": \"SYF\",\n",
    "    \"TENCENT HOLDINGS LTD\": \"0700.HK\",\n",
    "    \"WALMART INC\": \"WMT\",\n",
    "    \"WIX.COM LTD\": \"WIX\",\n",
    "    \"YEAHKA LTD\": \"9923.HK\",\n",
    "    \"ZILLOW GROUP INC - C\": \"Z\",\n",
    "    \"ZIP CO LTD\": \"ZIP.AX\",\n",
    "    \"CAPITAL ONE FINANCIAL CORP\": \"COF\",\n",
    "    \"CCC INTELLIGENT SOLUTIONS HO\": \"CCCS\",\n",
    "    \"CIELO SA\": \"CIEL3.SA\",\n",
    "    \"CIPHER MINING INC\": \"CIFR\",\n",
    "    \"CLEANSPARK INC\": \"CLSK\",\n",
    "    \"DISCOVERY LTD\": \"DSY.JO\",\n",
    "    \"DOCUSIGN INC\": \"DOCU\",\n",
    "    \"EML PAYMENTS LTD\": \"EML.AX\",\n",
    "    \"EPAM SYSTEMS INC\": \"EPAM\",\n",
    "    \"EQUIFAX INC\": \"EFX\",\n",
    "    \"EVERQUOTE INC - CLASS A\": \"EVER\",\n",
    "    \"FUTU HOLDINGS LTD-ADR\": \"FUTU\",\n",
    "    \"HDFC BANK LIMITED\": \"HDB\",\n",
    "    \"INTERACTIVE BROKERS GRO-CL A\": \"IBKR\",\n",
    "    \"IRESS LTD\": \"IRE.AX\",\n",
    "    \"JPMORGAN CHASE & CO\": \"JPM\",\n",
    "    \"LINKLOGIS INC-CLASS B\": \"9959.HK\",\n",
    "    \"LUFAX HOLDING LTD-ADR\": \"LU\",\n",
    "    \"MARA HOLDINGS INC\": \"MARA\",\n",
    "    \"MOODY'S CORP\": \"MCO\",\n",
    "    \"MSCI INC\": \"MSCI\",\n",
    "    \"ONESTREAM INC\": \"ONES\",\n",
    "    \"PDD HOLDINGS INC\": \"PDD\",\n",
    "    \"QUDIAN INC-SPON ADR\": \"QD\",\n",
    "    \"RIOT PLATFORMS INC\": \"RIOT\",\n",
    "    \"S&P GLOBAL INC\": \"SPGI\",\n",
    "    \"SAPIENS INTERNATIONAL CORP\": \"SPNS\",\n",
    "    \"STRATEGY INC\": \"MSTR\",\n",
    "    \"THOMSON REUTERS CORP\": \"TRI\",\n",
    "    \"UP FINTECH HOLDING LTD - ADR\": \"TIGR\",\n",
    "    \"VIRTU FINANCIAL INC-CLASS A\": \"VIRT\",\n",
    "    \"WORKDAY INC-CLASS A\": \"WDAY\",\n",
    "    \"WORKIVA INC\": \"WK\",\n",
    "    \"ZSCALER INC\": \"ZS\",\n",
    "}\n",
    "\n",
    "# ============== 2) Name/Ticker DataFrame ==============\n",
    "name_ticker_df = pd.DataFrame(\n",
    "    [{\"Name\": n, \"Ticker\": t} for n, t in NAME_TO_TICKER.items()]\n",
    ")\n",
    "\n",
    "# ============== 3) æŠ“å–ç‰¹å¾ï¼ˆæ¥è‡ª .infoï¼‰/ Fetch rich features ==============\n",
    "def fetch_rich_features(ticker: str) -> Optional[dict]:\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info if hasattr(t, \"info\") else {}\n",
    "        if not info:\n",
    "            print(f\"[WARN] {ticker}: .info is empty\")\n",
    "            return None\n",
    "\n",
    "        mktcap = info.get(\"marketCap\")\n",
    "        feature = {\n",
    "            \"Ticker\": ticker,\n",
    "            # Size & valuation\n",
    "            \"market_cap\": mktcap,\n",
    "            \"log_mktcap\": np.log(mktcap) if mktcap and mktcap > 0 else np.nan,\n",
    "            \"enterprise_value\": info.get(\"enterpriseValue\"),\n",
    "            \"pe_trailing\": info.get(\"trailingPE\"),\n",
    "            \"pe_forward\": info.get(\"forwardPE\"),\n",
    "            \"pb\": info.get(\"priceToBook\"),\n",
    "            \"ps_trailing\": info.get(\"priceToSalesTrailing12Months\"),\n",
    "            \"enterprise_to_revenue\": info.get(\"enterpriseToRevenue\"),\n",
    "            \"enterprise_to_ebitda\": info.get(\"enterpriseToEbitda\"),\n",
    "            # Profitability\n",
    "            \"profit_margins\": info.get(\"profitMargins\"),\n",
    "            \"gross_margins\": info.get(\"grossMargins\"),\n",
    "            \"operating_margins\": info.get(\"operatingMargins\"),\n",
    "            \"return_on_assets\": info.get(\"returnOnAssets\"),\n",
    "            \"return_on_equity\": info.get(\"returnOnEquity\"),\n",
    "            # Financial health\n",
    "            \"total_debt\": info.get(\"totalDebt\"),\n",
    "            \"debt_to_equity\": info.get(\"debtToEquity\"),\n",
    "            \"current_ratio\": info.get(\"currentRatio\"),\n",
    "            \"quick_ratio\": info.get(\"quickRatio\"),\n",
    "            # Growth & ops\n",
    "            \"revenue_growth\": info.get(\"revenueGrowth\"),\n",
    "            \"gross_profits\": info.get(\"grossProfits\"),\n",
    "            # Dividends\n",
    "            \"dividend_yield\": info.get(\"dividendYield\"),\n",
    "            \"payout_ratio\": info.get(\"payoutRatio\"),\n",
    "        }\n",
    "        # normalize None -> NaN\n",
    "        return {k: (np.nan if v is None else v) for k, v in feature.items()}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"Total names: {len(name_ticker_df)}\")\n",
    "\n",
    "rows, todo_names = [], []\n",
    "for _, r in name_ticker_df.iterrows():\n",
    "    name, tick = r[\"Name\"], r[\"Ticker\"]\n",
    "    if pd.isna(tick) or not str(tick).strip():\n",
    "        todo_names.append(name); continue\n",
    "    tkr = str(tick).strip()\n",
    "    print(f\"Fetching: {name} ({tkr}) ...\")\n",
    "    feats = fetch_rich_features(tkr)\n",
    "    if feats:\n",
    "        feats[\"Name\"] = name\n",
    "        rows.append(feats)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "features_df = pd.DataFrame(rows)\n",
    "print(f\"\\n[OK] Got features for {features_df.shape[0]} tickers.\")\n",
    "\n",
    "if todo_names:\n",
    "    print(\"\\n[TODO] Missing tickers (please fill mapping):\")\n",
    "    for nm in todo_names: print(\" -\", nm)\n",
    "\n",
    "# ============== 4) é€‰æ‹©ç‰¹å¾ + ç¼ºå¤±å€¼å¤„ç† + æ ‡å‡†åŒ– / Feature set & preprocessing ==============\n",
    "# ä½ å¯ä»¥æŒ‰éœ€è°ƒæ•´ç‰¹å¾åˆ— / Adjust features as needed\n",
    "feature_cols = [\n",
    "    \"log_mktcap\",\"pe_trailing\",\"pe_forward\",\"pb\",\"ps_trailing\",\n",
    "    \"enterprise_to_revenue\",\"enterprise_to_ebitda\",\n",
    "    \"profit_margins\",\"gross_margins\",\"operating_margins\",\n",
    "    \"return_on_assets\",\"return_on_equity\",\n",
    "    \"debt_to_equity\",\"current_ratio\",\"quick_ratio\",\n",
    "    \"revenue_growth\",\"dividend_yield\",\"payout_ratio\"\n",
    "]\n",
    "\n",
    "avail_cols = [c for c in feature_cols if c in features_df.columns]\n",
    "X_raw = features_df[avail_cols].copy()\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imp = imputer.fit_transform(X_raw)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "# ============== 5) è‡ªåŠ¨é€‰æ‹© Kï¼ˆè½®å»“ç³»æ•°ï¼‰/ Auto-select K via silhouette ==============\n",
    "n = X_scaled.shape[0]\n",
    "k_best, s_best = None, -1\n",
    "candidates = [2,3,4,5,6,7,8]\n",
    "\n",
    "if n >= 10:\n",
    "    for k in candidates:\n",
    "        if k >= n: continue\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "        labels = km.fit_predict(X_scaled)\n",
    "        try:\n",
    "            s = silhouette_score(X_scaled, labels)\n",
    "        except Exception:\n",
    "            s = -1\n",
    "        if s > s_best:\n",
    "            k_best, s_best = k, s\n",
    "else:\n",
    "    k_best = min(3, max(2, n-1))\n",
    "\n",
    "print(f\"\\n[Info] Selected K = {k_best} (silhouette={s_best:.4f} ; samples={n})\")\n",
    "\n",
    "# ============== 6) è®­ç»ƒ K-Means å¹¶è¾“å‡º / Fit & Output ==============\n",
    "kmeans = KMeans(n_clusters=k_best, n_init=50, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "out_df = features_df.copy()\n",
    "out_df[\"Cluster\"] = labels\n",
    "\n",
    "# cluster summary (å‡å€¼ä»…ä½œå‚è€ƒ)\n",
    "summary = (\n",
    "    out_df.groupby(\"Cluster\")[avail_cols]\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    "    .sort_values(\"Cluster\")\n",
    ")\n",
    "\n",
    "# cluster members\n",
    "members = (\n",
    "    out_df.sort_values([\"Cluster\", \"Name\"])\n",
    "    .groupby(\"Cluster\")[\"Name\"]\n",
    "    .apply(lambda s: \", \".join(s.tolist()))\n",
    "    .reset_index(name=\"NamesInCluster\")\n",
    ")\n",
    "\n",
    "summary = summary.merge(members, on=\"Cluster\", how=\"left\")\n",
    "\n",
    "# æ˜ç»†åˆ— / Detail columns\n",
    "detail_cols = [\"Name\",\"Ticker\"] + avail_cols + [\"Cluster\"]\n",
    "details = out_df[detail_cols].sort_values([\"Cluster\",\"Name\"]).reset_index(drop=True)\n",
    "\n",
    "# ============== 7) å¯¼å‡º / Export ==============\n",
    "summary_file = \"cluster_summary.xlsx\"\n",
    "with pd.ExcelWriter(summary_file, engine=\"openpyxl\") as w:\n",
    "    details.to_excel(w, sheet_name=\"Details\", index=False)\n",
    "    summary.to_excel(w, sheet_name=\"ClusterSummary\", index=False)\n",
    "    name_ticker_df.to_excel(w, sheet_name=\"NameTickerMap\", index=False)\n",
    "\n",
    "details.to_csv(\"peer_groups_details.csv\", index=False, encoding=\"utf-8\")\n",
    "summary.to_csv(\"peer_groups_summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n[DONE] Exported:\\n - {summary_file}\\n - peer_groups_details.csv\\n - peer_groups_summary.csv\")\n",
    "\n",
    "# ============== 8) å‹å¥½æ‰“å° / Friendly print ==============\n",
    "def fmt(x):\n",
    "    try:\n",
    "        if pd.isna(x): return \"NA\"\n",
    "        return f\"{x:,.3f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "print(\"\\n=== Cluster Summary ===\")\n",
    "print(summary[[\"Cluster\"] + avail_cols + [\"NamesInCluster\"]])\n",
    "\n",
    "print(\"\\n=== Cluster Members (sample) ===\")\n",
    "for c, g in details.groupby(\"Cluster\"):\n",
    "    print(f\"\\nCluster {c}:\")\n",
    "    for _, r in g.iterrows():\n",
    "        print(f\" - {r['Name']} ({r['Ticker']})\")\n"
   ],
   "id": "dfe9a6b0c03db837",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 148\n",
      "Fetching: BLOCK INC (SQ) ...\n",
      "[ERROR] SQ: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: PAYPAL HOLDINGS INC (PYPL) ...\n",
      "[ERROR] PYPL: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: SHIFT4 PAYMENTS INC-CLASS A (FOUR) ...\n",
      "[ERROR] FOUR: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: STONECO LTD-A (STNE) ...\n",
      "[ERROR] STNE: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: AFFIRM HOLDINGS INC (AFRM) ...\n",
      "[ERROR] AFRM: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: BILL HOLDINGS INC (BILL) ...\n",
      "[ERROR] BILL: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: FIDELITY NATIONAL INFO SERV (FIS) ...\n",
      "[ERROR] FIS: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: FISERV INC (FI) ...\n",
      "[ERROR] FI: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: JACK HENRY & ASSOCIATES INC (JKHY) ...\n",
      "[ERROR] JKHY: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: LEMONADE INC (LMND) ...\n",
      "[ERROR] LMND: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: NCINO INC (NCNO) ...\n",
      "[ERROR] NCNO: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: PAGSEGURO DIGITAL LTD-CL A (PAGS) ...\n",
      "[ERROR] PAGS: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: SOFI TECHNOLOGIES INC (SOFI) ...\n",
      "[ERROR] SOFI: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: TOAST INC-CLASS A (TOST) ...\n",
      "[ERROR] TOST: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: ACI WORLDWIDE INC (ACIW) ...\n",
      "[ERROR] ACIW: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: BLACKLINE INC (BL) ...\n",
      "[ERROR] BL: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: BROADRIDGE FINANCIAL SOLUTIO (BR) ...\n",
      "[ERROR] BR: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: COINBASE GLOBAL INC -CLASS A (COIN) ...\n",
      "[ERROR] COIN: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: DLOCAL LTD (DLO) ...\n",
      "[ERROR] DLO: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: FLYWIRE CORP-VOTING (FLYW) ...\n",
      "[ERROR] FLYW: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: GLOBAL PAYMENTS INC (GPN) ...\n",
      "[ERROR] GPN: Too Many Requests. Rate limited. Try after a while.\n",
      "Fetching: GUIDEWIRE SOFTWARE INC (GWRE) ...\n",
      "[ERROR] GWRE: Too Many Requests. Rate limited. Try after a while.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 235\u001B[39m\n\u001B[32m    233\u001B[39m         feats[\u001B[33m\"\u001B[39m\u001B[33mName\u001B[39m\u001B[33m\"\u001B[39m] = name\n\u001B[32m    234\u001B[39m         rows.append(feats)\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    237\u001B[39m features_df = pd.DataFrame(rows)\n\u001B[32m    238\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m[OK] Got features for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfeatures_df.shape[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m tickers.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
